## Day one: Thursday 5 July

I am working on deploying machine learning algorithm that helps predict if breast cancer is cancerous or not.

* I have the data set
* I have prepared the notebook
* I have done pre-data processing

### Thoughts: I am just a beginner at this but I am really engaging myself to grow so far so good.
### Link: <a href = "https://github.com/atwine/100-Days-Of-ML-Code/blob/master/Week%209%20Major%20Project.ipynb">commit</a>


## Day two: Friday 6th July

Today I worked on Data processing and feature selection in my cancer project:
The aim is to find which are the best features that i can use in the machine learning algorithm for the best predictive results

Some of the things I want to do are:
* Deploy SVM, KNN and KMeans
* Use PCA to reduce dimensions
* Normalize results of the data I have before I deploy the models

<<<<<<< HEAD
### Link: <a href = "https://github.com/atwine/100-Days-Of-ML-Code/blob/master/Week%209%20Major%20Project.ipynb">commit</a>

### Thoughts: Just really excited to see what I can do over the period that I have spent learning ML


## Day three: Sat 7th July 2018

Today I am going to look at deploying Decision tree algorithms for a dataset that I got from Kaggle. This dataset was given by a bank to assist predict which  candidates are able to pay their loans.

* Deploy Decision Tree
* Make my first submission on kaggle for the results I get

>> #### this is going to be an interesting time, i still feel like I have things mixed up though the other reason why this is the time to make things better.

I am alittle stuck though: how do i extract into csv the information for the kmeans classifier to deal with it? I feel somewhat confused.


## Day four: Sun 8th July 2018

Today I watched a tutorial from Raj on implementing linear regression technique from scratch.

I am doing some studying on different algorithms: <a href = "https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/">link</a>

I forked some  github repository to assist me learn more about these algorithm implementations: <a href = "https://github.com/atwine/machine-learning">link</a>

Reviewed machine learning implementations of peers in edx.org

## Day five: Mon 9th July 2018

=======
### Thoughts: Just really excited to see what I can do over the period that I have spent learning ML

Today I am taking time to look through various algorithms as much as possible and also to practice my programming. I had a question on how I could produce a resulting dataset but now I have the answer to it.

### I predict in the near days I will be able to do much more seeing that I have kinda made it through the novice stages.

I have looked at these today:
* 1. Linear Regression
* 2. Logistics regression
* 3. Got an sklearn cheat sheet which drew a great picture on how the info works.
* 4. Decision tree algorithm
* 5. Support Vector Machines
* 6. Read a chapter collective intelligence.pdf

>> what is amazing is that the decision tree algorithm was able to classify the information 100% correct which is quite amazing in comparison to what i have seen so far.


## Day six: Tue 10th July 2018

Today is a continuation of ML algorithms
* 1. Naive Bayes.
* 2. KNN.
* 3. Kmeans.
* 4. Random Forest.
* 5. Tuning algorithms <a href= 'https://www.jeremyjordan.me/hyperparameter-tuning/'>Tuning Hyperparamenters</a>
* 6. Random Forest

I am learning so much, I am glad.
Today landed on the notion of tuning algorithms, I am getting the clearer picture day by day.


## Day 7: Wed 11th July:

Studied: - how to read research a paper by rajraval
         - how to read math equations
         - signed up for introduction to mathematical thinking by Stanford Uni

Hyper-parameter testing:
* <a href ='http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf'>Link </a>
* <a href ='http://scikit-learn.org/stable/modules/grid_search.html'>Link </a>
* <a href ='https://blog.sigopt.com/posts/common-problems-in-hyperparameter-optimization'>Link </a>
* <a href ='http://betatim.github.io/posts/bayesian-hyperparameter-search/'>Link </a>
* <a href ='https://thuijskens.github.io/2016/12/29/bayesian-optimisation/'>Link </a>
* <a href ='https://thuijskens.github.io/2017/05/12/pydata-london/'>Link </a>


## Day 8: Thur 12th July:

Today I want to look at parameter tuning and hyper parameter tuning
 * - <a href = 'http://scikit-learn.org/stable/modules/grid_search.html'>Hyperparameter Tuning</a>
 * - Dimension reduction Algorithms
 * - Gradient boosting Algorithm <a href = 'https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/'>Hyper Parameter Tuning XGBoost</a>
 * - Light Gradient Boosting Algorithm
 <a href = 'http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters'> Parameter tuning</a>


 ## Day 9:  Fri 13th July:

 Today I forked some work on parameter tuning for XGBoost aglorithms
 Got some GitHub repos, which I am going through a cell at a time.

## Day 10-11:  Sat 14th July- Sun 15th July:
 Studied various articles on handling data:
 1- <a href ='https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python' >Cool Article</a>
 2- <a href ='https://www.kaggle.com/nanomathias/feature-engineering-importance-testing' >2</a>
 3- <a href ='https://www.kaggle.com/asindico/customer-segments-with-pca' >3</a>
 4- <a href ='https://www.kaggle.com/dansbecker/cross-validation' >4</a>
 5- <a href ='https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search' >Hyper parameter tuning</a>

 6- <a href = 'https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/'>CatBoost</a>

## Day 12 Mon 16th July:
 Today concentrated on going through my course: DataScience with Python from edx
 I also begun on a new course: Mathematical Thinking which will help me catch up with stat when I begin it. I want to take these two courses at the same time. one on Coursera and another on edx both about Math.

## Day 13 Mon 17th July:
 TODAY: I studied this article on the process of Machine learning I documented many steps on what it takes to get good results in the process of ML.

 THOUGHTS: I had not idea it was so cumbersome this process, stages of feature engineering and all, its quite hectic but a great learning process.
   <a href ='https://www.kaggle.com/atwine/start-here-a-gentle-introduction' >Commit</a>

   I recently handed in a submission on kaggle that won me 0.43 percent accuracy, I want to redo it after the manner which i have learnt, then i will gain a better accuracy.
   I will use xgboost, and do better data analysis on my kaggle dataset.

## Day 14 Wed 18th July

   I am continuing to catch up with strategically learning how to manage the ML process from 0 to hero from the guys who have done it before.
   I am hoping to get better at using some of these methods for competitions till I am good enough to win some competitions.
   I haven't finished the first article and there are some few also pending.

## Day 15 Thur  19th July
  Today I am studying mathematical thinking on Coursera
  I am also looking into feature creation as a great part of machine learning.
  I had issues with the work network, did not do much programming today.

## Day 16 Friday  20th July

Today I have some time to look into:
- Mathematical thinking from Stanford University
- I am also going to finish my tutorial on feature creation and also machine learning.
- tomorrow I have a finals exam on python for data DataScience so I have to prepare for it and begin to dive into the next module on math and statistics.

## Day 17 Saturday  21th July
- Earned my certificate in Python for DataScience
- begun my classes in statistics and probability


## Day 18 Monday  23th July
- looking at feature creation on kaggle as I get better at machine learning algorithms.
