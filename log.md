## Day one: Thursday 5 July

I am working on deploying machine learning algorithm that helps predict if breast cancer is cancerous or not.

* I have the data set
* I have prepared the notebook
* I have done pre-data processing

### Thoughts: I am just a beginner at this but I am really engaging myself to grow so far so good.
### Link: <a href = "https://github.com/atwine/100-Days-Of-ML-Code/blob/master/Week%209%20Major%20Project.ipynb">commit</a>


## Day two: Friday 6th July

Today I worked on Data processing and feature selection in my cancer project:
The aim is to find which are the best features that i can use in the machine learning algorithm for the best predictive results

Some of the things I want to do are:
* Deploy SVM, KNN and KMeans
* Use PCA to reduce dimensions
* Normalize results of the data I have before I deploy the models

<<<<<<< HEAD
### Link: <a href = "https://github.com/atwine/100-Days-Of-ML-Code/blob/master/Week%209%20Major%20Project.ipynb">commit</a>

### Thoughts: Just really excited to see what I can do over the period that I have spent learning ML


## Day three: Sat 7th July 2018

Today I am going to look at deploying Decision tree algorithms for a dataset that I got from Kaggle. This dataset was given by a bank to assist predict which  candidates are able to pay their loans.

* Deploy Decision Tree
* Make my first submission on kaggle for the results I get

>> #### this is going to be an interesting time, i still feel like I have things mixed up though the other reason why this is the time to make things better.


I am alittle stuck though: how do i extract into csv the information for the kmeans classifier to deal with it? I feel somewhat confused.


## Day four: Sun 8th July 2018

Today I watched a tutorial from Raj on implementing linear regression technique from scratch.

I am doing some studying on different algorithms: <a href = "https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/">link</a>

I forked some  github repository to assist me learn more about these algorithm implementations: <a href = "https://github.com/atwine/machine-learning">link</a>

Reviewed machine learning implementations of peers in edx.org

## Day five: Mon 9th July 2018

=======
### Thoughts: Just really excited to see what I can do over the period that I have spent learning ML

Today I am taking time to look through various algorithms as much as possible and also to practice my programming. I had a question on how I could produce a resulting dataset but now I have the answer to it.

### I predict in the near days I will be able to do much more seeing that I have kinda made it through the novice stages.

I have looked at these today:
* 1. Linear Regression
* 2. Logistics regression
* 3. Got an sklearn cheat sheet which drew a great picture on how the info works.
* 4. Decision tree algorithm
* 5. Support Vector Machines
* 6. Read a chapter collective intelligence.pdf

>> what is amazing is that the decision tree algorithm was able to classify the information 100% correct which is quite amazing in comparison to what i have seen so far.


## Day six: Tue 10th July 2018

Today is a continuation of ML algorithms
* 1. Naive Bayes.
* 2. KNN.
* 3. Kmeans.
* 4. Random Forest.
* 5. Tuning algorithms <a href= 'https://www.jeremyjordan.me/hyperparameter-tuning/'>Tuning Hyperparamenters</a>
* 6. Random Forest

I am learning so much, I am glad.
Today landed on the notion of tuning algorithms, I am getting the clearer picture day by day.


## Day 7: Wed 11th July:

Studied: - how to read research a paper by rajraval
         - how to read math equations
         - signed up for introduction to mathematical thinking by Stanford Uni

Hyper-parameter testing:
* <a href ='http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf'>Link </a>
* <a href ='http://scikit-learn.org/stable/modules/grid_search.html'>Link </a>
* <a href ='https://blog.sigopt.com/posts/common-problems-in-hyperparameter-optimization'>Link </a>
* <a href ='http://betatim.github.io/posts/bayesian-hyperparameter-search/'>Link </a>
* <a href ='https://thuijskens.github.io/2016/12/29/bayesian-optimisation/'>Link </a>
* <a href ='https://thuijskens.github.io/2017/05/12/pydata-london/'>Link </a>


## Day 8: Thur 12th July:

Today I want to look at parameter tuning and hyper parameter tuning
 * - <a href = 'http://scikit-learn.org/stable/modules/grid_search.html'>Hyperparameter Tuning</a>
 * - Dimension reduction Algorithms
 * - Gradient boosting Algorithm <a href = 'https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/'>Hyper Parameter Tuning XGBoost</a>
 * - Light Gradient Boosting Algorithm
 <a href = 'http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters'> Parameter tuning</a>


 ## Day 9:  Fri 13th July:

 Today I forked some work on parameter tuning for XGBoost aglorithms
 Got some GitHub repos, which I am going through a cell at a time.

## Day 10-11:  Sat 14th July- Sun 15th July:
 Studied various articles on handling data:
 1- <a href ='https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python' >Cool Article</a>
 2- <a href ='https://www.kaggle.com/nanomathias/feature-engineering-importance-testing' >2</a>
 3- <a href ='https://www.kaggle.com/asindico/customer-segments-with-pca' >3</a>
 4- <a href ='https://www.kaggle.com/dansbecker/cross-validation' >4</a>
 5- <a href ='https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search' >Hyper parameter tuning</a>

 6- <a href = 'https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/'>CatBoost</a>

## Day 12 Mon 16th July:
 Today concentrated on going through my course: DataScience with Python from edx
 I also begun on a new course: Mathematical Thinking which will help me catch up with stat when I begin it. I want to take these two courses at the same time. one on Coursera and another on edx both about Math.

## Day 13 Mon 17th July:
 TODAY: I studied this article on the process of Machine learning I documented many steps on what it takes to get good results in the process of ML.

 THOUGHTS: I had not idea it was so cumbersome this process, stages of feature engineering and all, its quite hectic but a great learning process.
   <a href ='https://www.kaggle.com/atwine/start-here-a-gentle-introduction' >Commit</a>

   I recently handed in a submission on kaggle that won me 0.43 percent accuracy, I want to redo it after the manner which i have learnt, then i will gain a better accuracy.
   I will use xgboost, and do better data analysis on my kaggle dataset.

## Day 14 Wed 18th July

   I am continuing to catch up with strategically learning how to manage the ML process from 0 to hero from the guys who have done it before.
   I am hoping to get better at using some of these methods for competitions till I am good enough to win some competitions.
   I haven't finished the first article and there are some few also pending.

## Day 15 Thur  19th July
  Today I am studying mathematical thinking on Coursera
  I am also looking into feature creation as a great part of machine learning.
  I had issues with the work network, did not do much programming today.

## Day 16 Friday  20th July

Today I have some time to look into:
- Mathematical thinking from Stanford University
- I am also going to finish my tutorial on feature creation and also machine learning.
- tomorrow I have a finals exam on python for data DataScience so I have to prepare for it and begin to dive into the next module on math and statistics.

## Day 17 Saturday  21th July
- Earned my certificate in Python for DataScience
- begun my classes in statistics and probability


## Day 18 Monday  23th July
- looking at feature creation on kaggle as I get better at machine learning algorithms.
- [looked at automated feature engineering, featuretools modules](https://www.featurelabs.com/blog/deep-feature-synthesis/)
- [featuretools](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219)

## Day 19 Tue  24th July
- read an article on non parametric statistics
- still studying feature creation but i want to dive into practicing what I learnt on the home loan competition on kaggle also other competitions
- I am also continuing with my classes on statitics and probability with python, where I am meeting new suprises.
- scratched the surface on pytorch and neural networks, I may need to learn more about these soon.
- Automated Hyperparameter tuning using Hyperopt

###THOUGHTS: in order to be good at this one has to continue learning and practicing the fundamentals of what they know till its part of you.


## Day 20 Wed  25th July
- made some applicationf for data science positions
- Studied loss function choosing criteria
-[automated hyperparameter tuning](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)
- THOUGHTS: today I learnt that, once you have cleaned your data and prepared it for training, you need to tune your algorithm of choice, and one of the methods you do so is by using a loss function, what a loss function does is it helps you get the correct values of x for a particular function(ml algorithm) where the los function is equal to 0, so these values are then used to train the model if we want to get accuracy.
[Artificial Neural Networks](https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75)

## Day 21 Wed  26th July
I have looked at the inner parts of the algorithms but if someone asked me what is SVP, i know how to deploy it but describing it in words is kind of difficult, therefore I am looking at the definitions and getting the bigger picture of some of these Algorithms
[SVM](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)
[Tree Based Models](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
[Nive Bayes](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)
[Ensemble Modeling](https://www.analyticsvidhya.com/blog/2015/09/questions-ensemble-modeling/)

## Day 22 Monday  30th July
- Today i did some basic HTML
- I want to redo my home loan competition based on the things I have learnt.
- Watched rajraval video on the kaggle competition on duration of taxis

## Day 23 Tue  31th July
Data exploration for my second kaggle handin attempt done today.
I am also going to dive into feature engineering then try model tuning

## Day 24 Wed  1st Aug

Today I am looking into feature engineering for my kaggle competitions
I am also doing my classes on statistics and probability with Python in my Data Science MicroMasters
Also reading up some articles on datacamp.com
Studying Set notation in Python


## Day 25 Thur 2nd Aug

Lookng into finalizing deploying my model and handing in some results on to kaggle.
Made my submission and my model skipped from 0.42 to 0.65 I want to reach at least 0.7,

Thoughts: I think there are some methods I should look into that help modify how my model behaves
I will soon jump into deep learning, I feel it will make the training process better, so i will use the basic models as the first way I do it then make it better.


## Day 26 Tue 7th Aug
I have not had my laptop for the last few days so my progress has been slow or none

Today i am continuing my classes on edx statistics and probabiliy with python looking at Set theory
Watched siraj's video on lgistic regression

Some of the learning links got from Raj's video

 More Learning Resources:
[Link 1 ](https://github.com/awesomedata/awesome-public-datasets)
[Link 2](http://www.statisticssolutions.com/what-is-logistic-regression/)
[Link 3](https://codesachin.wordpress.com/2015/08/16/logistic-regression-for-dummies/)
[Link 4](https://www.youtube.com/watch?v=zAULhNrnuL4)
[Link 5](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)
[Link 6](https://towardsdatascience.com/the-logistic-regression-algorithm-75fe48e21cfa)

[Github Link to the code](https://towardsdatascience.com/the-logistic-regression-algorithm-75fe48e21cfa)

Logistic regression measures the relationship between categorical dependent variables and one or more independent variables, it is more robust than linear regression to outliers in the data

## Day 27 Thur 9 Aug

I just recovered from a PC issue, that took my 4 days off:
Today: I want to
1 - Continue through the study on ML projec by will koerhsen
2 - If possible complete the second module in my statistics class

[Handling Missing Values](https://www.tandfonline.com/doi/full/10.1080/1743727X.2014.979146)
<br>
[Error Evaluation](https://www.coursera.org/lecture/machine-learning-projects/single-number-evaluation-metric-wIKkC)
<br>
[TPOT](https://github.com/EpistasisLab/tpot) - This is a machine learning algorithm that automates ML by taking in a data set and doing an automated run to provide the best data pipelines that can help in solving the problem.
<br>
[Cross Validation](https://machinelearningmastery.com/k-fold-cross-validation/) - I just wanted to really understand what cross validation is about.
<br>
[Over fitting Vs Underfitting](https://elitedatascience.com/overfitting-in-machine-learning)

## Day 28 Friday 10th Aug
Today I am reading some articles as has become my habit since I begun the challenge.
I am also completing my classes on sets specifically cartesian products.

#Note Time Series:
### Found some articles on time series I need to read later
[1](https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/)
<br>
[2](https://machinelearningmastery.com/taxonomy-of-time-series-forecasting-problems/)
<br>
[3](https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/)
<br>


## Day 29 Monday 13th Aug
I have gone through the class on sets and done the programming excercises too.
I want to deploy TPOT for the Home Credit Default Risk ML Challenge.

## Day 30 Tue 14th Aug
Continue to deploy the TPOT model for autoML
Submit a file to the kaggle competition to see how the TPOT module helps out in ML
[Commit](https://www.kaggle.com/atwine/home-credit-default-risk-using-tpot)

Completed my AWS account for machine learning and I run my instance for TPOT, I will also run my instance for featuretools later.

Practiced deploying an instance on AWS

## Day 31 Wed 15th Aug

I run multiple instances of T, it has given me several pipelines

Today i want to run them all and see what results they give me in comparison with what I already submitted in the Kaggle competition


## Day 32 Thursday 15th Aug

Today I run the instances that i had run in TPOT
I did not realize any improvement in what I submitted after using tpot
I suspect i did not use it well, because I see tendencies of overfitting

## Day 33 Sunday 19th Aug

Today I set up a repository for our school of AI on github so we can keep in touch with all we do.

## Day 34 Mon 20th Aug

Today I am finalizing with some ML methods, I want to quickly jump into Deep learning on fast.ai
I want to do one more round of Jupyter notebook and hand in with 0.7 accuracy just to go through the processing
I want to learn to use featuretools


## Day 35 Tue 21th Aug

Studying about deep feature synthesis under feature featuretools [Link](https://www.featurelabs.com/blog/deep-feature-synthesis/)

THOUGHTS: mastering this feature engineering thing is quite something, I may need to give it some more time than I expected. I landed on some notebooks from the featuretools lab. They are a handful

## Day 36 Wed 22th Aug

Today I am dealing with featuretools, studying it up a bit and implementing it because I want to begin on Deep Learning tomorrow.

I am going to follow the set up by Mr. willkoehrsen on Kaggle.

I am also looking at manual feature engineering from the same teacher.

## Day 37 Wed 23th Aug

Caught up with loads of work:

Read some articles on machine learning, learning never stops. willkoehrsen again with some nice articles I can work with.

## Day 38 Wed 24th Aug

Today I am going to practice with the Titanic dataset all that I have been learning.
I am going to do:
1- Data analysis
2- Finding missing Values
3- Encoding missing Values
4- Finding how features relate
5- Feature engineering
6- Deploying base models
7- handing in the csv after practice.

Finished today's tasks and got my ML model to predict at 0.7 accuracy
I am proud because i went through the thing myself, I think I am now learning the process more and more
