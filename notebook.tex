
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Week 9 Major Project}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Using machine learning algorithms to explore a data set and
predict the type of breast cancer someone
has:}\label{using-machine-learning-algorithms-to-explore-a-data-set-and-predict-the-type-of-breast-cancer-someone-has}

    By Atwine Mugume Twinamatsiko.

    \subsubsection{Motivation:}\label{motivation}

Breast cancer is one of the leading cancers that cause death in women.
In this study, I want to deploy machine learning algorithm to see if the
diagnosis of breast cancer can be predicted based on some initial
variables.

    The data used in this analysis is from UCI library of machine learning

https://archive.ics.uci.edu/ml/datasets.html?format=\&task=cla\&att=num\&area=life\&numAtt=\&numIns=\&type=mvar\&sort=nameUp\&view=table

    \subsection{Objective:}\label{objective}

\begin{quote}
\subsubsection{To test to what percentage can predictive analysis of
breast cancer tumor data aid in correct
diagnosis}\label{to-test-to-what-percentage-can-predictive-analysis-of-breast-cancer-tumor-data-aid-in-correct-diagnosis}
\end{quote}

    \paragraph{Honor Society of Nursing
(STTI)}\label{honor-society-of-nursing-stti}

\subparagraph{Administration}\label{administration}

\begin{quote}
"Breast cancer is a serious disease. Although some types of breast
cancer are curable - especially in the early stages - treatment can be
long and difficult. Some breast cancers are less curable, especially
once they spread to other body parts. About 20 percent of breast cancer
patients die of the disease."
\end{quote}

    \paragraph{Stuart A. Linder, MD}\label{stuart-a.-linder-md}

\subparagraph{Plastic Surgery}\label{plastic-surgery}

\begin{quote}
\begin{quote}
"Breast cancer can be serious; however, the majority of cases are
curable if diagnosed early on. Statistics indicate 20\% of patients who
have breast cancer will succumb from their disease. Approximately 1 out
of 8 women will develop a form of breast cancer and, therefore,
diagnostic screening, including mammogram, ultrasounds, MRIs, and
biopsies of palpable nodes, nodules, or masses should be performed."
\end{quote}
\end{quote}

    \begin{quote}
Prospective identification of tumorigenic breast cancer cells Muhammad
Al-Hajj, Max S. Wicha, Adalberto Benito-Hernandez, Sean J. Morrison, and
Michael F. Clarke PNAS April 1, 2003. 100 (7) 3983-3988;
https://doi.org/10.1073/pnas.0530291100 Communicated by Jack E. Dixon,
University of Michigan Medical School, Ann Arbor, MI (received for
review December 18, 2002)
\end{quote}

\begin{quote}
\begin{quote}
Abstract Breast cancer is the most common malignancy in United States
women, accounting for \textgreater{}40,000 deaths each year. These
breast tumors are comprised of phenotypically diverse populations of
breast cancer cells. Using a model in which human breast cancer cells
were grown in immunocompromised mice, we found that only a minority of
breast cancer cells had the ability to form new tumors. We were able to
distinguish the tumorigenic (tumor initiating) from the nontumorigenic
cancer cells based on cell surface marker expression.
\end{quote}
\end{quote}

    \paragraph{At this stage we are have these
objectives:}\label{at-this-stage-we-are-have-these-objectives}

\begin{itemize}
\tightlist
\item
  Ingest the data
\item
  Check for missing values and deal with them
\item
  Explore the data with descriptive statistics to see the shape of the
  data we are dealing with
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{c+c1}{\PYZsh{}Importing the necessary modules}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         
         \PY{c+c1}{\PYZsh{}Load libraries for data processing}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} \PY{c+c1}{\PYZsh{}data processing, CSV file I/O (e.g. pd.read\PYZus{}csv)}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{import} \PY{n}{norm}
         \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns} \PY{c+c1}{\PYZsh{} visualization}
         
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)} 
         \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{axes.titlesize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{large}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    Now after calling the required libraries in python, let us igest the
data from our local location

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./breastCancer.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Lest take at the top ten rows in our data

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}68}]:}          id diagnosis  radius\_mean  texture\_mean  perimeter\_mean  area\_mean  \textbackslash{}
         0    842302         M        17.99         10.38          122.80     1001.0   
         1    842517         M        20.57         17.77          132.90     1326.0   
         2  84300903         M        19.69         21.25          130.00     1203.0   
         3  84348301         M        11.42         20.38           77.58      386.1   
         4  84358402         M        20.29         14.34          135.10     1297.0   
         5    843786         M        12.45         15.70           82.57      477.1   
         6    844359         M        18.25         19.98          119.60     1040.0   
         7  84458202         M        13.71         20.83           90.20      577.9   
         8    844981         M        13.00         21.82           87.50      519.8   
         9  84501001         M        12.46         24.04           83.97      475.9   
         
            smoothness\_mean  compactness\_mean  concavity\_mean  concave points\_mean  \textbackslash{}
         0          0.11840           0.27760         0.30010              0.14710   
         1          0.08474           0.07864         0.08690              0.07017   
         2          0.10960           0.15990         0.19740              0.12790   
         3          0.14250           0.28390         0.24140              0.10520   
         4          0.10030           0.13280         0.19800              0.10430   
         5          0.12780           0.17000         0.15780              0.08089   
         6          0.09463           0.10900         0.11270              0.07400   
         7          0.11890           0.16450         0.09366              0.05985   
         8          0.12730           0.19320         0.18590              0.09353   
         9          0.11860           0.23960         0.22730              0.08543   
         
                     {\ldots}             radius\_worst  texture\_worst  perimeter\_worst  \textbackslash{}
         0           {\ldots}                    25.38          17.33           184.60   
         1           {\ldots}                    24.99          23.41           158.80   
         2           {\ldots}                    23.57          25.53           152.50   
         3           {\ldots}                    14.91          26.50            98.87   
         4           {\ldots}                    22.54          16.67           152.20   
         5           {\ldots}                    15.47          23.75           103.40   
         6           {\ldots}                    22.88          27.66           153.20   
         7           {\ldots}                    17.06          28.14           110.60   
         8           {\ldots}                    15.49          30.73           106.20   
         9           {\ldots}                    15.09          40.68            97.65   
         
            area\_worst  smoothness\_worst  compactness\_worst  concavity\_worst  \textbackslash{}
         0      2019.0            0.1622             0.6656           0.7119   
         1      1956.0            0.1238             0.1866           0.2416   
         2      1709.0            0.1444             0.4245           0.4504   
         3       567.7            0.2098             0.8663           0.6869   
         4      1575.0            0.1374             0.2050           0.4000   
         5       741.6            0.1791             0.5249           0.5355   
         6      1606.0            0.1442             0.2576           0.3784   
         7       897.0            0.1654             0.3682           0.2678   
         8       739.3            0.1703             0.5401           0.5390   
         9       711.4            0.1853             1.0580           1.1050   
         
            concave points\_worst  symmetry\_worst  fractal\_dimension\_worst  
         0                0.2654          0.4601                  0.11890  
         1                0.1860          0.2750                  0.08902  
         2                0.2430          0.3613                  0.08758  
         3                0.2575          0.6638                  0.17300  
         4                0.1625          0.2364                  0.07678  
         5                0.1741          0.3985                  0.12440  
         6                0.1932          0.3063                  0.08368  
         7                0.1556          0.3196                  0.11510  
         8                0.2060          0.4378                  0.10720  
         9                0.2210          0.4366                  0.20750  
         
         [10 rows x 32 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{c+c1}{\PYZsh{}let us see how big our data is by finding its total features and variables}
         \PY{n}{np}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{data}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}69}]:} (569, 32)
\end{Verbatim}
            
    Our data has 569 rows and 32 variables that we have to interact with

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{c+c1}{\PYZsh{}lets get some more information about the data contained in the dataframe}
         \PY{n}{data}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 569 entries, 0 to 568
Data columns (total 32 columns):
id                         569 non-null int64
diagnosis                  569 non-null object
radius\_mean                569 non-null float64
texture\_mean               569 non-null float64
perimeter\_mean             569 non-null float64
area\_mean                  569 non-null float64
smoothness\_mean            569 non-null float64
compactness\_mean           569 non-null float64
concavity\_mean             569 non-null float64
concave points\_mean        569 non-null float64
symmetry\_mean              569 non-null float64
fractal\_dimension\_mean     569 non-null float64
radius\_se                  569 non-null float64
texture\_se                 569 non-null float64
perimeter\_se               569 non-null float64
area\_se                    569 non-null float64
smoothness\_se              569 non-null float64
compactness\_se             569 non-null float64
concavity\_se               569 non-null float64
concave points\_se          569 non-null float64
symmetry\_se                569 non-null float64
fractal\_dimension\_se       569 non-null float64
radius\_worst               569 non-null float64
texture\_worst              569 non-null float64
perimeter\_worst            569 non-null float64
area\_worst                 569 non-null float64
smoothness\_worst           569 non-null float64
compactness\_worst          569 non-null float64
concavity\_worst            569 non-null float64
concave points\_worst       569 non-null float64
symmetry\_worst             569 non-null float64
fractal\_dimension\_worst    569 non-null float64
dtypes: float64(30), int64(1), object(1)
memory usage: 142.3+ KB

    \end{Verbatim}

    From the cell above we can see that the data is majorly float type and
we have no null values. However to be sure let us check.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{c+c1}{\PYZsh{}let us check for the null variables if there are any at all.}
         \PY{n}{data}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}71}]:} 0
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{c+c1}{\PYZsh{}to see some of the descriptive stats we can call the describe function on the dataframe}
         \PY{n}{data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}72}]:}                  id  radius\_mean  texture\_mean  perimeter\_mean    area\_mean  \textbackslash{}
         count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   
         mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   
         std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   
         min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   
         25\%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   
         50\%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   
         75\%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   
         max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   
         
                smoothness\_mean  compactness\_mean  concavity\_mean  concave points\_mean  \textbackslash{}
         count       569.000000        569.000000      569.000000           569.000000   
         mean          0.096360          0.104341        0.088799             0.048919   
         std           0.014064          0.052813        0.079720             0.038803   
         min           0.052630          0.019380        0.000000             0.000000   
         25\%           0.086370          0.064920        0.029560             0.020310   
         50\%           0.095870          0.092630        0.061540             0.033500   
         75\%           0.105300          0.130400        0.130700             0.074000   
         max           0.163400          0.345400        0.426800             0.201200   
         
                symmetry\_mean           {\ldots}             radius\_worst  texture\_worst  \textbackslash{}
         count     569.000000           {\ldots}               569.000000     569.000000   
         mean        0.181162           {\ldots}                16.269190      25.677223   
         std         0.027414           {\ldots}                 4.833242       6.146258   
         min         0.106000           {\ldots}                 7.930000      12.020000   
         25\%         0.161900           {\ldots}                13.010000      21.080000   
         50\%         0.179200           {\ldots}                14.970000      25.410000   
         75\%         0.195700           {\ldots}                18.790000      29.720000   
         max         0.304000           {\ldots}                36.040000      49.540000   
         
                perimeter\_worst   area\_worst  smoothness\_worst  compactness\_worst  \textbackslash{}
         count       569.000000   569.000000        569.000000         569.000000   
         mean        107.261213   880.583128          0.132369           0.254265   
         std          33.602542   569.356993          0.022832           0.157336   
         min          50.410000   185.200000          0.071170           0.027290   
         25\%          84.110000   515.300000          0.116600           0.147200   
         50\%          97.660000   686.500000          0.131300           0.211900   
         75\%         125.400000  1084.000000          0.146000           0.339100   
         max         251.200000  4254.000000          0.222600           1.058000   
         
                concavity\_worst  concave points\_worst  symmetry\_worst  \textbackslash{}
         count       569.000000            569.000000      569.000000   
         mean          0.272188              0.114606        0.290076   
         std           0.208624              0.065732        0.061867   
         min           0.000000              0.000000        0.156500   
         25\%           0.114500              0.064930        0.250400   
         50\%           0.226700              0.099930        0.282200   
         75\%           0.382900              0.161400        0.317900   
         max           1.252000              0.291000        0.663800   
         
                fractal\_dimension\_worst  
         count               569.000000  
         mean                  0.083946  
         std                   0.018061  
         min                   0.055040  
         25\%                   0.071460  
         50\%                   0.080040  
         75\%                   0.092080  
         max                   0.207500  
         
         [8 rows x 31 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{c+c1}{\PYZsh{}we can also look at how skewed the data is, with a positive value meaning the variable is skewed tot he right while }
         \PY{c+c1}{\PYZsh{}negative means its is skewed to the left}
         \PY{n}{data}\PY{o}{.}\PY{n}{skew}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}73}]:} id                         6.473752
         radius\_mean                0.942380
         texture\_mean               0.650450
         perimeter\_mean             0.990650
         area\_mean                  1.645732
         smoothness\_mean            0.456324
         compactness\_mean           1.190123
         concavity\_mean             1.401180
         concave points\_mean        1.171180
         symmetry\_mean              0.725609
         fractal\_dimension\_mean     1.304489
         radius\_se                  3.088612
         texture\_se                 1.646444
         perimeter\_se               3.443615
         area\_se                    5.447186
         smoothness\_se              2.314450
         compactness\_se             1.902221
         concavity\_se               5.110463
         concave points\_se          1.444678
         symmetry\_se                2.195133
         fractal\_dimension\_se       3.923969
         radius\_worst               1.103115
         texture\_worst              0.498321
         perimeter\_worst            1.128164
         area\_worst                 1.859373
         smoothness\_worst           0.415426
         compactness\_worst          1.473555
         concavity\_worst            1.150237
         concave points\_worst       0.492616
         symmetry\_worst             1.433928
         fractal\_dimension\_worst    1.662579
         dtype: float64
\end{Verbatim}
            
    From the data in the previous cell we can see that most of the variables
are skewed to the right.

The best way to notice such is through visualization such as below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}74}]:}          id diagnosis  radius\_mean  texture\_mean  perimeter\_mean  area\_mean  \textbackslash{}
         0    842302         M        17.99         10.38          122.80     1001.0   
         1    842517         M        20.57         17.77          132.90     1326.0   
         2  84300903         M        19.69         21.25          130.00     1203.0   
         3  84348301         M        11.42         20.38           77.58      386.1   
         4  84358402         M        20.29         14.34          135.10     1297.0   
         
            smoothness\_mean  compactness\_mean  concavity\_mean  concave points\_mean  \textbackslash{}
         0          0.11840           0.27760          0.3001              0.14710   
         1          0.08474           0.07864          0.0869              0.07017   
         2          0.10960           0.15990          0.1974              0.12790   
         3          0.14250           0.28390          0.2414              0.10520   
         4          0.10030           0.13280          0.1980              0.10430   
         
                     {\ldots}             radius\_worst  texture\_worst  perimeter\_worst  \textbackslash{}
         0           {\ldots}                    25.38          17.33           184.60   
         1           {\ldots}                    24.99          23.41           158.80   
         2           {\ldots}                    23.57          25.53           152.50   
         3           {\ldots}                    14.91          26.50            98.87   
         4           {\ldots}                    22.54          16.67           152.20   
         
            area\_worst  smoothness\_worst  compactness\_worst  concavity\_worst  \textbackslash{}
         0      2019.0            0.1622             0.6656           0.7119   
         1      1956.0            0.1238             0.1866           0.2416   
         2      1709.0            0.1444             0.4245           0.4504   
         3       567.7            0.2098             0.8663           0.6869   
         4      1575.0            0.1374             0.2050           0.4000   
         
            concave points\_worst  symmetry\_worst  fractal\_dimension\_worst  
         0                0.2654          0.4601                  0.11890  
         1                0.1860          0.2750                  0.08902  
         2                0.2430          0.3613                  0.08758  
         3                0.2575          0.6638                  0.17300  
         4                0.1625          0.2364                  0.07678  
         
         [5 rows x 32 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{}there are some columns that are not so significant such as the id in this case we are dropping in.}
         \PY{n}{new\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{c+c1}{\PYZsh{}let us check the new data}
         \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}76}]:}   diagnosis  radius\_mean  texture\_mean  perimeter\_mean  area\_mean  \textbackslash{}
         0         M        17.99         10.38          122.80     1001.0   
         1         M        20.57         17.77          132.90     1326.0   
         2         M        19.69         21.25          130.00     1203.0   
         3         M        11.42         20.38           77.58      386.1   
         4         M        20.29         14.34          135.10     1297.0   
         
            smoothness\_mean  compactness\_mean  concavity\_mean  concave points\_mean  \textbackslash{}
         0          0.11840           0.27760          0.3001              0.14710   
         1          0.08474           0.07864          0.0869              0.07017   
         2          0.10960           0.15990          0.1974              0.12790   
         3          0.14250           0.28390          0.2414              0.10520   
         4          0.10030           0.13280          0.1980              0.10430   
         
            symmetry\_mean           {\ldots}             radius\_worst  texture\_worst  \textbackslash{}
         0         0.2419           {\ldots}                    25.38          17.33   
         1         0.1812           {\ldots}                    24.99          23.41   
         2         0.2069           {\ldots}                    23.57          25.53   
         3         0.2597           {\ldots}                    14.91          26.50   
         4         0.1809           {\ldots}                    22.54          16.67   
         
            perimeter\_worst  area\_worst  smoothness\_worst  compactness\_worst  \textbackslash{}
         0           184.60      2019.0            0.1622             0.6656   
         1           158.80      1956.0            0.1238             0.1866   
         2           152.50      1709.0            0.1444             0.4245   
         3            98.87       567.7            0.2098             0.8663   
         4           152.20      1575.0            0.1374             0.2050   
         
            concavity\_worst  concave points\_worst  symmetry\_worst  \textbackslash{}
         0           0.7119                0.2654          0.4601   
         1           0.2416                0.1860          0.2750   
         2           0.4504                0.2430          0.3613   
         3           0.6869                0.2575          0.6638   
         4           0.4000                0.1625          0.2364   
         
            fractal\_dimension\_worst  
         0                  0.11890  
         1                  0.08902  
         2                  0.08758  
         3                  0.17300  
         4                  0.07678  
         
         [5 rows x 31 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{c+c1}{\PYZsh{}the features should have redused to 31 now.}
         \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}77}]:} (569, 31)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{c+c1}{\PYZsh{}let us plot a pairwise view of a few columns}
         \PY{n}{slice\PYZus{}} \PY{o}{=} \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{slice\PYZus{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Volumes/BackUP/anaconda3/lib/python3.6/site-packages/ipykernel\_launcher.py:2: DeprecationWarning: 
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing

See the documentation here:
http://pandas.pydata.org/pandas-docs/stable/indexing.html\#ix-indexer-is-deprecated
  

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}78}]:} <seaborn.axisgrid.PairGrid at 0x1a237c6fd0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{c+c1}{\PYZsh{}To see specifically the distribution of the data we are going to sample 10 columns and visualize those to see the trend}
         \PY{n}{slice\PYZus{}1} \PY{o}{=} \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
         \PY{n}{hist} \PY{o}{=} \PY{n}{slice\PYZus{}1}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,}\PY{n}{grid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The cell visualization above is a picture view of the function we run
earlier (data.skew) we are able to see the distribution of the data,
this helps us to decide if its necesarry to apply functions such as log
to make the data into a bess shape for analysis or any other factors
that may help later when we deploy the model

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{c+c1}{\PYZsh{}we can also use density plots to look into the data and see how its distributed}
         \PY{n}{plt} \PY{o}{=} \PY{n}{slice\PYZus{}1}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{density}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{subplots}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} 
                              \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}\PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{c+c1}{\PYZsh{}we are slicing a piece of the full data set because the visualizations would be so many we have many columns}
         \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}81}]:} Index(['diagnosis', 'radius\_mean', 'texture\_mean', 'perimeter\_mean',
                'area\_mean', 'smoothness\_mean', 'compactness\_mean', 'concavity\_mean',
                'concave points\_mean', 'symmetry\_mean', 'fractal\_dimension\_mean',
                'radius\_se', 'texture\_se', 'perimeter\_se', 'area\_se', 'smoothness\_se',
                'compactness\_se', 'concavity\_se', 'concave points\_se', 'symmetry\_se',
                'fractal\_dimension\_se', 'radius\_worst', 'texture\_worst',
                'perimeter\_worst', 'area\_worst', 'smoothness\_worst',
                'compactness\_worst', 'concavity\_worst', 'concave points\_worst',
                'symmetry\_worst', 'fractal\_dimension\_worst'],
               dtype='object')
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{c+c1}{\PYZsh{}one other way to see how the data looks like is to use boxplots, they help communicate, variance mean and also shows}
         \PY{c+c1}{\PYZsh{}outliers in the data}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mf}{11.7}\PY{p}{,}\PY{l+m+mf}{8.27}
         \PY{n}{box} \PY{o}{=} \PY{n}{slice\PYZus{}1}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{box}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{n}{subplots}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}\PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{c+c1}{\PYZsh{}in using PCA we will need to increase or reduce the dimensions based on the correlation of the variables and their variance.}
         \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{slice\PYZus{}1}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}83}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1a256c6f60>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Feature Selection:}\label{feature-selection}

\begin{quote}
This stage is very crucial in a way that,choosing the wrong features
affects the results of the model that you would want to deploy. As a
result we want to know some issues:
\end{quote}

\begin{itemize}
\tightlist
\item
  what type of variables to we have (how many are categorical)
\item
  if we had na values how would we deal with them
\item
  How many dimensions are we going to deal with and which are best
  suited for this study
\item
  Deploy the models we want to use for the study
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{c+c1}{\PYZsh{}what variables do we have in our data}
         \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 569 entries, 0 to 568
Data columns (total 31 columns):
diagnosis                  569 non-null object
radius\_mean                569 non-null float64
texture\_mean               569 non-null float64
perimeter\_mean             569 non-null float64
area\_mean                  569 non-null float64
smoothness\_mean            569 non-null float64
compactness\_mean           569 non-null float64
concavity\_mean             569 non-null float64
concave points\_mean        569 non-null float64
symmetry\_mean              569 non-null float64
fractal\_dimension\_mean     569 non-null float64
radius\_se                  569 non-null float64
texture\_se                 569 non-null float64
perimeter\_se               569 non-null float64
area\_se                    569 non-null float64
smoothness\_se              569 non-null float64
compactness\_se             569 non-null float64
concavity\_se               569 non-null float64
concave points\_se          569 non-null float64
symmetry\_se                569 non-null float64
fractal\_dimension\_se       569 non-null float64
radius\_worst               569 non-null float64
texture\_worst              569 non-null float64
perimeter\_worst            569 non-null float64
area\_worst                 569 non-null float64
smoothness\_worst           569 non-null float64
compactness\_worst          569 non-null float64
concavity\_worst            569 non-null float64
concave points\_worst       569 non-null float64
symmetry\_worst             569 non-null float64
fractal\_dimension\_worst    569 non-null float64
dtypes: float64(30), object(1)
memory usage: 137.9+ KB

    \end{Verbatim}

    From the values in the cell above we can see that we don't have many
categorical variables, this tells us that we have to creat dummies or
convert to 0 and 1 values for the models to understand the diagnosis
variable.

Let us now separate the variables into the features we want and the
predition values

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}85}]:}   diagnosis  radius\_mean  texture\_mean  perimeter\_mean  area\_mean  \textbackslash{}
         0         M        17.99         10.38          122.80     1001.0   
         1         M        20.57         17.77          132.90     1326.0   
         2         M        19.69         21.25          130.00     1203.0   
         3         M        11.42         20.38           77.58      386.1   
         4         M        20.29         14.34          135.10     1297.0   
         
            smoothness\_mean  compactness\_mean  concavity\_mean  concave points\_mean  \textbackslash{}
         0          0.11840           0.27760          0.3001              0.14710   
         1          0.08474           0.07864          0.0869              0.07017   
         2          0.10960           0.15990          0.1974              0.12790   
         3          0.14250           0.28390          0.2414              0.10520   
         4          0.10030           0.13280          0.1980              0.10430   
         
            symmetry\_mean           {\ldots}             radius\_worst  texture\_worst  \textbackslash{}
         0         0.2419           {\ldots}                    25.38          17.33   
         1         0.1812           {\ldots}                    24.99          23.41   
         2         0.2069           {\ldots}                    23.57          25.53   
         3         0.2597           {\ldots}                    14.91          26.50   
         4         0.1809           {\ldots}                    22.54          16.67   
         
            perimeter\_worst  area\_worst  smoothness\_worst  compactness\_worst  \textbackslash{}
         0           184.60      2019.0            0.1622             0.6656   
         1           158.80      1956.0            0.1238             0.1866   
         2           152.50      1709.0            0.1444             0.4245   
         3            98.87       567.7            0.2098             0.8663   
         4           152.20      1575.0            0.1374             0.2050   
         
            concavity\_worst  concave points\_worst  symmetry\_worst  \textbackslash{}
         0           0.7119                0.2654          0.4601   
         1           0.2416                0.1860          0.2750   
         2           0.4504                0.2430          0.3613   
         3           0.6869                0.2575          0.6638   
         4           0.4000                0.1625          0.2364   
         
            fractal\_dimension\_worst  
         0                  0.11890  
         1                  0.08902  
         2                  0.08758  
         3                  0.17300  
         4                  0.07678  
         
         [5 rows x 31 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{n}{df} \PY{o}{=} \PY{n}{new\PYZus{}data}\PY{o}{.}\PY{n}{values}
         \PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{31}\PY{p}{]} \PY{c+c1}{\PYZsh{}these are the features we are going to use in the training and predictions}
         \PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{}these are the predicted values}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{c+c1}{\PYZsh{}since y is categorical we need to encode the values with label encoders}
         \PY{n}{y}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}87}]:} array(['M', 'M', 'M', 'M', 'M'], dtype=object)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{c+c1}{\PYZsh{}transform the class labels from their original string representation (M and B) into integers}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{LabelEncoder}
         \PY{n}{le} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{le}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{c+c1}{\PYZsh{}now the values of y are 0 or 1}
         \PY{n}{y}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}89}]:} array([1, 1, 1, 1, 1])
\end{Verbatim}
            
    \subsection{Feature interaction}\label{feature-interaction}

\begin{quote}
Feature interaction with how the features are related to one another, in
terms of variance and correlation, at this level we would like to take
on the features that are highly relatable and drop those that are not as
related as we want in order to get a higher percentage on the score on
predictability.
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{c+c1}{\PYZsh{}lets split the data:}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}Split data set in train 75\PYZpc{} and test 25\PYZpc{}}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{)}
         \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}90}]:} ((426, 30), (426,), (143, 30), (143,))
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
         
         \PY{c+c1}{\PYZsh{} Normalize the  data (center around 0 and scale to remove the variance).}
         \PY{n}{scaler} \PY{o}{=}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
         \PY{n}{Xs} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Volumes/BackUP/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{n}{Xs}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}92}]:}          0         1         2         3         4         5         6   \textbackslash{}
         0  1.097064 -2.073335  1.269934  0.984375  1.568466  3.283515  2.652874   
         1  1.829821 -0.353632  1.685955  1.908708 -0.826962 -0.487072 -0.023846   
         2  1.579888  0.456187  1.566503  1.558884  0.942210  1.052926  1.363478   
         3 -0.768909  0.253732 -0.592687 -0.764464  3.283553  3.402909  1.915897   
         4  1.750297 -1.151816  1.776573  1.826229  0.280372  0.539340  1.371011   
         
                  7         8         9     {\ldots}           20        21        22  \textbackslash{}
         0  2.532475  2.217515  2.255747    {\ldots}     1.886690 -1.359293  2.303601   
         1  0.548144  0.001392 -0.868652    {\ldots}     1.805927 -0.369203  1.535126   
         2  2.037231  0.939685 -0.398008    {\ldots}     1.511870 -0.023974  1.347475   
         3  1.451707  2.867383  4.910919    {\ldots}    -0.281464  0.133984 -0.249939   
         4  1.428493 -0.009560 -0.562450    {\ldots}     1.298575 -1.466770  1.338539   
         
                  23        24        25        26        27        28        29  
         0  2.001237  1.307686  2.616665  2.109526  2.296076  2.750622  1.937015  
         1  1.890489 -0.375612 -0.430444 -0.146749  1.087084 -0.243890  0.281190  
         2  1.456285  0.527407  1.082932  0.854974  1.955000  1.152255  0.201391  
         3 -0.550021  3.394275  3.893397  1.989588  2.175786  6.046041  4.935010  
         4  1.220724  0.220556 -0.313395  0.613179  0.729259 -0.868353 -0.397100  
         
         [5 rows x 30 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{c+c1}{\PYZsh{}we want to see how the points are distributed now below}
         \PY{c+c1}{\PYZsh{} we will find a stardard distribution along what could be a normal gradient }
         \PY{n}{v} \PY{o}{=} \PY{n}{Xs}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{u} \PY{o}{=} \PY{n}{Xs}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{v}\PY{p}{,}\PY{n}{u}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}93}]:} <matplotlib.collections.PathCollection at 0x1a25a14cc0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{c+c1}{\PYZsh{}since we have many features lets use the module for feature selection in sklearn to do our selection}
         \PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}selection} \PY{k}{as} \PY{n+nn}{skf}
         
         \PY{n}{select} \PY{o}{=} \PY{n}{skf}\PY{o}{.}\PY{n}{SelectKBest}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{selected\PYZus{}features} \PY{o}{=} \PY{n}{select}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{indices\PYZus{}selected} \PY{o}{=} \PY{n}{selected\PYZus{}features}\PY{o}{.}\PY{n}{get\PYZus{}support}\PY{p}{(}\PY{n}{indices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{colnames\PYZus{}selected} \PY{o}{=} \PY{p}{[}\PY{n}{Xs}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{indices\PYZus{}selected}\PY{p}{]}
         
         \PY{n}{X\PYZus{}train\PYZus{}selected} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{colnames\PYZus{}selected}\PY{p}{]}
         \PY{n}{X\PYZus{}test\PYZus{}selected} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{n}{colnames\PYZus{}selected}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{n}{X\PYZus{}train\PYZus{}selected}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}selected}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}95}]:} ((20, 30), (20, 30))
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{colnames\PYZus{}selected}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[0, 1, 2, 3, 5, 6, 7, 10, 12, 13, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{c+c1}{\PYZsh{} Function to build model and find model performance}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{roc\PYZus{}auc\PYZus{}score}
         
         \PY{k}{def} \PY{n+nf}{performance}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{:}
             \PY{n}{model} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{]}
             \PY{n}{auc} \PY{o}{=} \PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}hat}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{auc}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}98}]:} ((426,), (143,))
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{c+c1}{\PYZsh{} \PYZsh{} Find performance of model using preprocessed data}
          \PY{c+c1}{\PYZsh{} auc\PYZus{}processed = find\PYZus{}model\PYZus{}perf(X\PYZus{}train\PYZus{}selected, y\PYZus{}train, X\PYZus{}test\PYZus{}selected, y\PYZus{}test)}
          \PY{c+c1}{\PYZsh{} print(auc\PYZus{}processed)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:}  \PY{c+c1}{\PYZsh{}Create an SVM classifier and train it on 70\PYZpc{} of the data set.}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
          \PY{n}{clf} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{probability}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}Analyze accuracy of predictions on 30\PYZpc{} of the holdout test sample.}
          \PY{n}{classifier\PYZus{}score} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
          \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{The classifier accuracy score is }\PY{l+s+si}{\PYZob{}:03.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{classifier\PYZus{}score}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

The classifier accuracy score is 0.69


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}110}]:} \PY{c+c1}{\PYZsh{}the accuracy score is quite low lets reduce the dimensions with PCA}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
          \PY{c+c1}{\PYZsh{} feature extraction}
          \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
          \PY{n}{fit} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xs}\PY{p}{)}
\end{Verbatim}


    How many components should we retain?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}112}]:} \PY{n}{var}\PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}
          \PY{n}{var}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}112}]:} array([0.44272026, 0.18971182, 0.09393163, 0.06602135, 0.05495768,
                 0.04024522, 0.02250734, 0.01588724, 0.01389649, 0.01168978])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{var}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PCA Elbow Choice}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{c+c1}{\PYZsh{}we pick the components at the elbow = 2}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}115}]:} [<matplotlib.lines.Line2D at 0x1a2097f860>]
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}116}]:} \PY{c+c1}{\PYZsh{} Divide records in training and testing sets.}
          \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{Xs}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Create an SVM classifier and train it on 70\PYZpc{} of the data set.}
          \PY{n}{clf} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{probability}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}Analyze accuracy of predictions on 30\PYZpc{} of the holdout test sample.}
          \PY{n}{classifier\PYZus{}score} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
          \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{The classifier accuracy score is }\PY{l+s+si}{\PYZob{}:03.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{classifier\PYZus{}score}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

The classifier accuracy score is 0.95


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
          \PY{c+c1}{\PYZsh{}since we are using many features we can use cross validations to take a component of the features in order to get how the model performs.}
          \PY{n}{n\PYZus{}folds} \PY{o}{=} \PY{l+m+mi}{3}
          \PY{n}{cv\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{SVC}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{Xs}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{n\PYZus{}folds}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{The }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZhy{}fold cross\PYZhy{}validation accuracy score for this classifier is }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}folds}\PY{p}{,} \PY{n}{cv\PYZus{}error}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

The 3-fold cross-validation accuracy score for this classifier is 0.97


    \end{Verbatim}

    The value above and beyond tell us we need a minimum of three valuables
to predict the resulting diagnosis at the accuracy of 97\%

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}124}]:} \PY{c+c1}{\PYZsh{} The confusion matrix helps visualize the performance of the algorithm.}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
          \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
          \PY{n}{cm} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{cm}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[106   1]
 [  7  57]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}127}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{classification\PYZus{}report}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred} \PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
             precision    recall  f1-score   support

          0       0.94      0.99      0.96       107
          1       0.98      0.89      0.93        64

avg / total       0.95      0.95      0.95       171


    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
