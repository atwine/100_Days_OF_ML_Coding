{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essentials of Machine Learning Algorithms (with Python)\n",
    "\n",
    "<a href= 'https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/'>Link to the tutorial</a>\n",
    "\n",
    "Broadly, there are 3 types of Machine Learning Algorithms..\n",
    "\n",
    "1. Supervised Learning\n",
    "How it works: This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.\n",
    "\n",
    "\n",
    "2. Unsupervised Learning\n",
    "How it works: In this algorithm, we do not have any target or outcome variable to predict / estimate.  It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning: Apriori algorithm, K-means.\n",
    "\n",
    "\n",
    "3. Reinforcement Learning:\n",
    "How it works:  Using this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning: Markov Decision Process\n",
    "\n",
    "\n",
    "### List of Common Machine Learning Algorithms\n",
    "Here is the list of commonly used machine learning algorithms. These algorithms can be applied to almost any data problem:\n",
    "\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Decision Tree\n",
    "* SVM\n",
    "* Naive Bayes\n",
    "* kNN\n",
    "* K-Means\n",
    "* Random Forest\n",
    "* Dimensionality Reduction Algorithms\n",
    "* Gradient Boosting algorithms\n",
    "* GBM\n",
    "* XGBoost\n",
    "* LightGBM\n",
    "* CatBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.\n",
    "\n",
    "The best way to understand linear regression is to relive this experience of childhood. Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* Y – Dependent Variable\n",
    "* a – Slope\n",
    "* X – Independent variable\n",
    "* b – Intercept\n",
    "These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.\n",
    "\n",
    "Look at the below example. Here we have identified the best fit line having linear equation y=0.2811x+13.9. Now using this equation, we can find the weight, knowing the height of a person.\n",
    "\n",
    "\n",
    "Linear Regression is of mainly two types: Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression is characterized by one independent variable. And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. While finding best fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us read the data\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am using a dataset \"breatCancer\" from UCI for these tests because it is already cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../breastCancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>843786</td>\n",
       "      <td>M</td>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>844359</td>\n",
       "      <td>M</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84458202</td>\n",
       "      <td>M</td>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>844981</td>\n",
       "      <td>M</td>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84501001</td>\n",
       "      <td>M</td>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "5    843786         M        12.45         15.70           82.57      477.1   \n",
       "6    844359         M        18.25         19.98          119.60     1040.0   \n",
       "7  84458202         M        13.71         20.83           90.20      577.9   \n",
       "8    844981         M        13.00         21.82           87.50      519.8   \n",
       "9  84501001         M        12.46         24.04           83.97      475.9   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760         0.30010              0.14710   \n",
       "1          0.08474           0.07864         0.08690              0.07017   \n",
       "2          0.10960           0.15990         0.19740              0.12790   \n",
       "3          0.14250           0.28390         0.24140              0.10520   \n",
       "4          0.10030           0.13280         0.19800              0.10430   \n",
       "5          0.12780           0.17000         0.15780              0.08089   \n",
       "6          0.09463           0.10900         0.11270              0.07400   \n",
       "7          0.11890           0.16450         0.09366              0.05985   \n",
       "8          0.12730           0.19320         0.18590              0.09353   \n",
       "9          0.11860           0.23960         0.22730              0.08543   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "5           ...                    15.47          23.75           103.40   \n",
       "6           ...                    22.88          27.66           153.20   \n",
       "7           ...                    17.06          28.14           110.60   \n",
       "8           ...                    15.49          30.73           106.20   \n",
       "9           ...                    15.09          40.68            97.65   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "5       741.6            0.1791             0.5249           0.5355   \n",
       "6      1606.0            0.1442             0.2576           0.3784   \n",
       "7       897.0            0.1654             0.3682           0.2678   \n",
       "8       739.3            0.1703             0.5401           0.5390   \n",
       "9       711.4            0.1853             1.0580           1.1050   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "5                0.1741          0.3985                  0.12440  \n",
       "6                0.1932          0.3063                  0.08368  \n",
       "7                0.1556          0.3196                  0.11510  \n",
       "8                0.2060          0.4378                  0.10720  \n",
       "9                0.2210          0.4366                  0.20750  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    M\n",
       "1    M\n",
       "2    M\n",
       "3    M\n",
       "4    M\n",
       "5    M\n",
       "6    M\n",
       "7    M\n",
       "8    M\n",
       "9    M\n",
       "Name: diagnosis, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the values are categorical so we need to transform them\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,2:29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is where I split the data so that it can fit into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 27), (398,), (171, 27), (171,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=7)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>11.660</td>\n",
       "      <td>17.07</td>\n",
       "      <td>73.70</td>\n",
       "      <td>421.0</td>\n",
       "      <td>0.07561</td>\n",
       "      <td>0.03630</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>0.1671</td>\n",
       "      <td>0.05731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.022160</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>13.28</td>\n",
       "      <td>19.74</td>\n",
       "      <td>83.61</td>\n",
       "      <td>542.5</td>\n",
       "      <td>0.09958</td>\n",
       "      <td>0.06476</td>\n",
       "      <td>0.03046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>15.530</td>\n",
       "      <td>33.56</td>\n",
       "      <td>103.70</td>\n",
       "      <td>744.9</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.16390</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.083990</td>\n",
       "      <td>0.2091</td>\n",
       "      <td>0.06650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>18.49</td>\n",
       "      <td>49.54</td>\n",
       "      <td>126.30</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>0.18830</td>\n",
       "      <td>0.55640</td>\n",
       "      <td>0.57030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>11.930</td>\n",
       "      <td>21.53</td>\n",
       "      <td>76.53</td>\n",
       "      <td>438.6</td>\n",
       "      <td>0.09768</td>\n",
       "      <td>0.07849</td>\n",
       "      <td>0.033280</td>\n",
       "      <td>0.020080</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.003856</td>\n",
       "      <td>13.67</td>\n",
       "      <td>26.15</td>\n",
       "      <td>87.54</td>\n",
       "      <td>583.0</td>\n",
       "      <td>0.15000</td>\n",
       "      <td>0.23990</td>\n",
       "      <td>0.15030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10.170</td>\n",
       "      <td>14.88</td>\n",
       "      <td>64.55</td>\n",
       "      <td>311.9</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.08061</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.06960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>0.041830</td>\n",
       "      <td>0.005953</td>\n",
       "      <td>11.02</td>\n",
       "      <td>17.45</td>\n",
       "      <td>69.86</td>\n",
       "      <td>368.6</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.02168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>11.890</td>\n",
       "      <td>17.36</td>\n",
       "      <td>76.20</td>\n",
       "      <td>435.6</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.07210</td>\n",
       "      <td>0.059290</td>\n",
       "      <td>0.074040</td>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.05875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.026780</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>12.40</td>\n",
       "      <td>18.99</td>\n",
       "      <td>79.46</td>\n",
       "      <td>472.4</td>\n",
       "      <td>0.13590</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0.07153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>18.220</td>\n",
       "      <td>18.70</td>\n",
       "      <td>120.30</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.14850</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.2092</td>\n",
       "      <td>0.06310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.026740</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>20.60</td>\n",
       "      <td>24.13</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>0.12800</td>\n",
       "      <td>0.22970</td>\n",
       "      <td>0.26230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>13.560</td>\n",
       "      <td>13.90</td>\n",
       "      <td>88.59</td>\n",
       "      <td>561.3</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.11920</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.044510</td>\n",
       "      <td>0.1962</td>\n",
       "      <td>0.06303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>14.98</td>\n",
       "      <td>17.13</td>\n",
       "      <td>101.10</td>\n",
       "      <td>686.6</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.26980</td>\n",
       "      <td>0.25770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>12.890</td>\n",
       "      <td>15.70</td>\n",
       "      <td>84.08</td>\n",
       "      <td>516.6</td>\n",
       "      <td>0.07818</td>\n",
       "      <td>0.09580</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>0.05935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>0.018780</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>13.90</td>\n",
       "      <td>19.69</td>\n",
       "      <td>92.12</td>\n",
       "      <td>595.6</td>\n",
       "      <td>0.09926</td>\n",
       "      <td>0.23170</td>\n",
       "      <td>0.33440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>11.800</td>\n",
       "      <td>16.58</td>\n",
       "      <td>78.99</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0.10910</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.074150</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.07371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>0.056280</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>13.74</td>\n",
       "      <td>26.38</td>\n",
       "      <td>91.93</td>\n",
       "      <td>591.7</td>\n",
       "      <td>0.13850</td>\n",
       "      <td>0.40920</td>\n",
       "      <td>0.45040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>12.160</td>\n",
       "      <td>18.03</td>\n",
       "      <td>78.29</td>\n",
       "      <td>455.3</td>\n",
       "      <td>0.09087</td>\n",
       "      <td>0.07838</td>\n",
       "      <td>0.029160</td>\n",
       "      <td>0.015270</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.06284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>13.34</td>\n",
       "      <td>27.87</td>\n",
       "      <td>88.83</td>\n",
       "      <td>547.4</td>\n",
       "      <td>0.12080</td>\n",
       "      <td>0.22790</td>\n",
       "      <td>0.16200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>16.240</td>\n",
       "      <td>18.77</td>\n",
       "      <td>108.80</td>\n",
       "      <td>805.1</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.18020</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.090520</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.06684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.013880</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>18.55</td>\n",
       "      <td>25.09</td>\n",
       "      <td>126.90</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>0.13650</td>\n",
       "      <td>0.47060</td>\n",
       "      <td>0.50260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>18.310</td>\n",
       "      <td>20.58</td>\n",
       "      <td>120.80</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>0.10680</td>\n",
       "      <td>0.12480</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.094510</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.05941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.015590</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>21.86</td>\n",
       "      <td>26.20</td>\n",
       "      <td>142.20</td>\n",
       "      <td>1493.0</td>\n",
       "      <td>0.14920</td>\n",
       "      <td>0.25360</td>\n",
       "      <td>0.37590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14.420</td>\n",
       "      <td>19.77</td>\n",
       "      <td>94.48</td>\n",
       "      <td>642.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.093880</td>\n",
       "      <td>0.058390</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.06390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014240</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>16.33</td>\n",
       "      <td>30.86</td>\n",
       "      <td>109.50</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.31940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>10.65</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>12.950</td>\n",
       "      <td>16.02</td>\n",
       "      <td>83.14</td>\n",
       "      <td>513.7</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>0.07943</td>\n",
       "      <td>0.061550</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.06470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011320</td>\n",
       "      <td>0.026250</td>\n",
       "      <td>0.004726</td>\n",
       "      <td>13.74</td>\n",
       "      <td>19.93</td>\n",
       "      <td>88.81</td>\n",
       "      <td>585.4</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.20680</td>\n",
       "      <td>0.22410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>13.150</td>\n",
       "      <td>15.34</td>\n",
       "      <td>85.31</td>\n",
       "      <td>538.9</td>\n",
       "      <td>0.09384</td>\n",
       "      <td>0.08498</td>\n",
       "      <td>0.092930</td>\n",
       "      <td>0.034830</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.06207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>14.77</td>\n",
       "      <td>20.50</td>\n",
       "      <td>97.67</td>\n",
       "      <td>677.3</td>\n",
       "      <td>0.14780</td>\n",
       "      <td>0.22560</td>\n",
       "      <td>0.30090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>13.460</td>\n",
       "      <td>28.21</td>\n",
       "      <td>85.89</td>\n",
       "      <td>562.1</td>\n",
       "      <td>0.07517</td>\n",
       "      <td>0.04726</td>\n",
       "      <td>0.012710</td>\n",
       "      <td>0.011170</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>0.05763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>14.69</td>\n",
       "      <td>35.63</td>\n",
       "      <td>97.11</td>\n",
       "      <td>680.6</td>\n",
       "      <td>0.11080</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.07934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>12.230</td>\n",
       "      <td>19.56</td>\n",
       "      <td>78.54</td>\n",
       "      <td>461.0</td>\n",
       "      <td>0.09586</td>\n",
       "      <td>0.08087</td>\n",
       "      <td>0.041870</td>\n",
       "      <td>0.041070</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.06013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.015030</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>14.44</td>\n",
       "      <td>28.36</td>\n",
       "      <td>92.15</td>\n",
       "      <td>638.4</td>\n",
       "      <td>0.14290</td>\n",
       "      <td>0.20420</td>\n",
       "      <td>0.13770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>17.750</td>\n",
       "      <td>28.03</td>\n",
       "      <td>117.30</td>\n",
       "      <td>981.6</td>\n",
       "      <td>0.09997</td>\n",
       "      <td>0.13140</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.082930</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.05916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>21.53</td>\n",
       "      <td>38.54</td>\n",
       "      <td>145.40</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.37620</td>\n",
       "      <td>0.63990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>13.820</td>\n",
       "      <td>24.49</td>\n",
       "      <td>92.33</td>\n",
       "      <td>595.9</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.16810</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.067590</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.07237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016160</td>\n",
       "      <td>0.024340</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>16.01</td>\n",
       "      <td>32.94</td>\n",
       "      <td>106.00</td>\n",
       "      <td>788.0</td>\n",
       "      <td>0.17940</td>\n",
       "      <td>0.39660</td>\n",
       "      <td>0.33810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>24.630</td>\n",
       "      <td>21.60</td>\n",
       "      <td>165.50</td>\n",
       "      <td>1841.0</td>\n",
       "      <td>0.10300</td>\n",
       "      <td>0.21060</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.1991</td>\n",
       "      <td>0.06739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015970</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>29.92</td>\n",
       "      <td>26.93</td>\n",
       "      <td>205.70</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>0.13420</td>\n",
       "      <td>0.41880</td>\n",
       "      <td>0.46580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>18.490</td>\n",
       "      <td>17.52</td>\n",
       "      <td>121.30</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>0.10120</td>\n",
       "      <td>0.13170</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.091830</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>0.06697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015730</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.88</td>\n",
       "      <td>146.40</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.14120</td>\n",
       "      <td>0.30890</td>\n",
       "      <td>0.35330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>25.220</td>\n",
       "      <td>24.91</td>\n",
       "      <td>171.50</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.26650</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.1829</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>30.00</td>\n",
       "      <td>33.62</td>\n",
       "      <td>211.70</td>\n",
       "      <td>2562.0</td>\n",
       "      <td>0.15730</td>\n",
       "      <td>0.60760</td>\n",
       "      <td>0.64760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>11.340</td>\n",
       "      <td>21.26</td>\n",
       "      <td>72.48</td>\n",
       "      <td>396.5</td>\n",
       "      <td>0.08759</td>\n",
       "      <td>0.06575</td>\n",
       "      <td>0.051330</td>\n",
       "      <td>0.018990</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.06529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>13.01</td>\n",
       "      <td>29.15</td>\n",
       "      <td>83.99</td>\n",
       "      <td>518.1</td>\n",
       "      <td>0.16990</td>\n",
       "      <td>0.21960</td>\n",
       "      <td>0.31200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>15.050</td>\n",
       "      <td>19.07</td>\n",
       "      <td>97.26</td>\n",
       "      <td>701.9</td>\n",
       "      <td>0.09215</td>\n",
       "      <td>0.08597</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>0.043350</td>\n",
       "      <td>0.1561</td>\n",
       "      <td>0.05915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>17.58</td>\n",
       "      <td>28.06</td>\n",
       "      <td>113.80</td>\n",
       "      <td>967.0</td>\n",
       "      <td>0.12460</td>\n",
       "      <td>0.21010</td>\n",
       "      <td>0.28660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.029810</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>16.84</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>13.640</td>\n",
       "      <td>15.60</td>\n",
       "      <td>87.38</td>\n",
       "      <td>575.3</td>\n",
       "      <td>0.09423</td>\n",
       "      <td>0.06630</td>\n",
       "      <td>0.047050</td>\n",
       "      <td>0.037310</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.05660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>14.85</td>\n",
       "      <td>19.05</td>\n",
       "      <td>94.11</td>\n",
       "      <td>683.4</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.12910</td>\n",
       "      <td>0.15330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>10.93</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>14.270</td>\n",
       "      <td>22.55</td>\n",
       "      <td>93.77</td>\n",
       "      <td>629.8</td>\n",
       "      <td>0.10380</td>\n",
       "      <td>0.11540</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.061390</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.05982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>0.014510</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>15.29</td>\n",
       "      <td>34.27</td>\n",
       "      <td>104.30</td>\n",
       "      <td>728.3</td>\n",
       "      <td>0.13800</td>\n",
       "      <td>0.27330</td>\n",
       "      <td>0.42340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>14.580</td>\n",
       "      <td>13.66</td>\n",
       "      <td>94.29</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.09832</td>\n",
       "      <td>0.08918</td>\n",
       "      <td>0.082220</td>\n",
       "      <td>0.043490</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.05640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>0.015390</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>16.76</td>\n",
       "      <td>17.24</td>\n",
       "      <td>108.50</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.19280</td>\n",
       "      <td>0.24920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>17.990</td>\n",
       "      <td>20.66</td>\n",
       "      <td>117.80</td>\n",
       "      <td>991.7</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.13040</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.088240</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.06069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>21.08</td>\n",
       "      <td>25.41</td>\n",
       "      <td>138.10</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.37350</td>\n",
       "      <td>0.33010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>11.92</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>15.780</td>\n",
       "      <td>22.91</td>\n",
       "      <td>105.70</td>\n",
       "      <td>782.6</td>\n",
       "      <td>0.11550</td>\n",
       "      <td>0.17520</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.094790</td>\n",
       "      <td>0.2096</td>\n",
       "      <td>0.07331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>20.19</td>\n",
       "      <td>30.50</td>\n",
       "      <td>130.30</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>0.18550</td>\n",
       "      <td>0.49250</td>\n",
       "      <td>0.73560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>12.220</td>\n",
       "      <td>20.04</td>\n",
       "      <td>79.47</td>\n",
       "      <td>453.1</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.11520</td>\n",
       "      <td>0.081750</td>\n",
       "      <td>0.021660</td>\n",
       "      <td>0.2124</td>\n",
       "      <td>0.06894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>13.16</td>\n",
       "      <td>24.17</td>\n",
       "      <td>85.13</td>\n",
       "      <td>515.3</td>\n",
       "      <td>0.14020</td>\n",
       "      <td>0.23150</td>\n",
       "      <td>0.35350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>13.660</td>\n",
       "      <td>19.13</td>\n",
       "      <td>89.46</td>\n",
       "      <td>575.3</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.096570</td>\n",
       "      <td>0.048120</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>0.06181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>15.14</td>\n",
       "      <td>25.50</td>\n",
       "      <td>101.40</td>\n",
       "      <td>708.8</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.31670</td>\n",
       "      <td>0.36600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>10.180</td>\n",
       "      <td>17.53</td>\n",
       "      <td>65.12</td>\n",
       "      <td>313.1</td>\n",
       "      <td>0.10610</td>\n",
       "      <td>0.08502</td>\n",
       "      <td>0.017680</td>\n",
       "      <td>0.019150</td>\n",
       "      <td>0.1910</td>\n",
       "      <td>0.06908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>0.026370</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>11.17</td>\n",
       "      <td>22.84</td>\n",
       "      <td>71.94</td>\n",
       "      <td>375.6</td>\n",
       "      <td>0.14060</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.06572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>14.950</td>\n",
       "      <td>17.57</td>\n",
       "      <td>96.85</td>\n",
       "      <td>678.1</td>\n",
       "      <td>0.11670</td>\n",
       "      <td>0.13050</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.086240</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.06216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028010</td>\n",
       "      <td>0.051680</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>18.55</td>\n",
       "      <td>21.43</td>\n",
       "      <td>121.40</td>\n",
       "      <td>971.4</td>\n",
       "      <td>0.14110</td>\n",
       "      <td>0.21640</td>\n",
       "      <td>0.33550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>11.900</td>\n",
       "      <td>14.65</td>\n",
       "      <td>78.11</td>\n",
       "      <td>432.8</td>\n",
       "      <td>0.11520</td>\n",
       "      <td>0.12960</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.030030</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.07839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.031270</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>13.15</td>\n",
       "      <td>16.51</td>\n",
       "      <td>86.26</td>\n",
       "      <td>509.6</td>\n",
       "      <td>0.14240</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.09420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>14.440</td>\n",
       "      <td>15.18</td>\n",
       "      <td>93.97</td>\n",
       "      <td>640.1</td>\n",
       "      <td>0.09970</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.084870</td>\n",
       "      <td>0.055320</td>\n",
       "      <td>0.1724</td>\n",
       "      <td>0.06081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>15.85</td>\n",
       "      <td>19.85</td>\n",
       "      <td>108.60</td>\n",
       "      <td>766.9</td>\n",
       "      <td>0.13160</td>\n",
       "      <td>0.27350</td>\n",
       "      <td>0.31030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>14.290</td>\n",
       "      <td>16.82</td>\n",
       "      <td>90.30</td>\n",
       "      <td>632.6</td>\n",
       "      <td>0.06429</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.05376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>14.91</td>\n",
       "      <td>20.65</td>\n",
       "      <td>94.44</td>\n",
       "      <td>684.6</td>\n",
       "      <td>0.08567</td>\n",
       "      <td>0.05036</td>\n",
       "      <td>0.03866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>13.770</td>\n",
       "      <td>13.27</td>\n",
       "      <td>88.06</td>\n",
       "      <td>582.7</td>\n",
       "      <td>0.09198</td>\n",
       "      <td>0.06221</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.05912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.021540</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>14.67</td>\n",
       "      <td>16.93</td>\n",
       "      <td>94.17</td>\n",
       "      <td>661.1</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.03732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>16.350</td>\n",
       "      <td>23.29</td>\n",
       "      <td>109.00</td>\n",
       "      <td>840.4</td>\n",
       "      <td>0.09742</td>\n",
       "      <td>0.14970</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.087730</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.06218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>19.38</td>\n",
       "      <td>31.03</td>\n",
       "      <td>129.30</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>0.14150</td>\n",
       "      <td>0.46650</td>\n",
       "      <td>0.70870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>11.890</td>\n",
       "      <td>18.35</td>\n",
       "      <td>77.32</td>\n",
       "      <td>432.2</td>\n",
       "      <td>0.09363</td>\n",
       "      <td>0.11540</td>\n",
       "      <td>0.066360</td>\n",
       "      <td>0.031420</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017850</td>\n",
       "      <td>0.027930</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>13.25</td>\n",
       "      <td>27.10</td>\n",
       "      <td>86.20</td>\n",
       "      <td>531.2</td>\n",
       "      <td>0.14050</td>\n",
       "      <td>0.30460</td>\n",
       "      <td>0.28060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>13.750</td>\n",
       "      <td>23.77</td>\n",
       "      <td>88.54</td>\n",
       "      <td>590.0</td>\n",
       "      <td>0.08043</td>\n",
       "      <td>0.06807</td>\n",
       "      <td>0.046970</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>0.1773</td>\n",
       "      <td>0.05429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>0.025980</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>15.01</td>\n",
       "      <td>26.34</td>\n",
       "      <td>98.00</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.09368</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.13590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>11.160</td>\n",
       "      <td>21.41</td>\n",
       "      <td>70.95</td>\n",
       "      <td>380.3</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.05978</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.06144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.026940</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>12.36</td>\n",
       "      <td>28.92</td>\n",
       "      <td>79.26</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.12820</td>\n",
       "      <td>0.11080</td>\n",
       "      <td>0.03582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015570</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>18.98</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>15.610</td>\n",
       "      <td>19.38</td>\n",
       "      <td>100.00</td>\n",
       "      <td>758.6</td>\n",
       "      <td>0.07840</td>\n",
       "      <td>0.05616</td>\n",
       "      <td>0.042090</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>0.05443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>17.91</td>\n",
       "      <td>31.67</td>\n",
       "      <td>115.90</td>\n",
       "      <td>988.6</td>\n",
       "      <td>0.10840</td>\n",
       "      <td>0.18070</td>\n",
       "      <td>0.22600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11.840</td>\n",
       "      <td>18.70</td>\n",
       "      <td>77.93</td>\n",
       "      <td>440.6</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>0.15160</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.051820</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.07799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.022730</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>16.82</td>\n",
       "      <td>28.12</td>\n",
       "      <td>119.40</td>\n",
       "      <td>888.7</td>\n",
       "      <td>0.16370</td>\n",
       "      <td>0.57750</td>\n",
       "      <td>0.69560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>16.140</td>\n",
       "      <td>14.86</td>\n",
       "      <td>104.30</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.09495</td>\n",
       "      <td>0.08501</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.045280</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.05875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008747</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>17.71</td>\n",
       "      <td>19.58</td>\n",
       "      <td>115.90</td>\n",
       "      <td>947.9</td>\n",
       "      <td>0.12060</td>\n",
       "      <td>0.17220</td>\n",
       "      <td>0.23100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>10.200</td>\n",
       "      <td>17.48</td>\n",
       "      <td>65.05</td>\n",
       "      <td>321.2</td>\n",
       "      <td>0.08054</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>0.057740</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.1964</td>\n",
       "      <td>0.06315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>11.48</td>\n",
       "      <td>24.47</td>\n",
       "      <td>75.40</td>\n",
       "      <td>403.7</td>\n",
       "      <td>0.09527</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.19250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.016780</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>14.50</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>10.510</td>\n",
       "      <td>20.19</td>\n",
       "      <td>68.64</td>\n",
       "      <td>334.2</td>\n",
       "      <td>0.11220</td>\n",
       "      <td>0.13030</td>\n",
       "      <td>0.064760</td>\n",
       "      <td>0.030680</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>0.07782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015440</td>\n",
       "      <td>0.022870</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>11.16</td>\n",
       "      <td>22.75</td>\n",
       "      <td>72.62</td>\n",
       "      <td>374.4</td>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.20490</td>\n",
       "      <td>0.12950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.030030</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>12.400</td>\n",
       "      <td>17.68</td>\n",
       "      <td>81.47</td>\n",
       "      <td>467.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.13160</td>\n",
       "      <td>0.077410</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.07102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011670</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>12.88</td>\n",
       "      <td>22.91</td>\n",
       "      <td>89.61</td>\n",
       "      <td>515.8</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.26290</td>\n",
       "      <td>0.24030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>11.740</td>\n",
       "      <td>14.02</td>\n",
       "      <td>74.24</td>\n",
       "      <td>427.3</td>\n",
       "      <td>0.07813</td>\n",
       "      <td>0.04340</td>\n",
       "      <td>0.022450</td>\n",
       "      <td>0.027630</td>\n",
       "      <td>0.2101</td>\n",
       "      <td>0.06113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018460</td>\n",
       "      <td>0.029210</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>13.31</td>\n",
       "      <td>18.26</td>\n",
       "      <td>84.70</td>\n",
       "      <td>533.7</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.08500</td>\n",
       "      <td>0.06735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>19.590</td>\n",
       "      <td>25.00</td>\n",
       "      <td>127.70</td>\n",
       "      <td>1191.0</td>\n",
       "      <td>0.10320</td>\n",
       "      <td>0.09871</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.090630</td>\n",
       "      <td>0.1663</td>\n",
       "      <td>0.05391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.016410</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>21.44</td>\n",
       "      <td>30.96</td>\n",
       "      <td>139.80</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>0.15280</td>\n",
       "      <td>0.18450</td>\n",
       "      <td>0.39770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>10.650</td>\n",
       "      <td>25.22</td>\n",
       "      <td>68.01</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.07234</td>\n",
       "      <td>0.023790</td>\n",
       "      <td>0.016150</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>0.06329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.021580</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>12.25</td>\n",
       "      <td>35.19</td>\n",
       "      <td>77.98</td>\n",
       "      <td>455.7</td>\n",
       "      <td>0.14990</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.11250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>19.890</td>\n",
       "      <td>20.26</td>\n",
       "      <td>130.50</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.13100</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.094310</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.06188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.010570</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>23.73</td>\n",
       "      <td>25.23</td>\n",
       "      <td>160.50</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.33090</td>\n",
       "      <td>0.41850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "350       11.660         17.07           73.70      421.0          0.07561   \n",
       "259       15.530         33.56          103.70      744.9          0.10630   \n",
       "115       11.930         21.53           76.53      438.6          0.09768   \n",
       "60        10.170         14.88           64.55      311.9          0.11340   \n",
       "275       11.890         17.36           76.20      435.6          0.12250   \n",
       "53        18.220         18.70          120.30     1033.0          0.11480   \n",
       "221       13.560         13.90           88.59      561.3          0.10510   \n",
       "284       12.890         15.70           84.08      516.6          0.07818   \n",
       "146       11.800         16.58           78.99      432.0          0.10910   \n",
       "480       12.160         18.03           78.29      455.3          0.09087   \n",
       "283       16.240         18.77          108.80      805.1          0.10660   \n",
       "516       18.310         20.58          120.80     1052.0          0.10680   \n",
       "99        14.420         19.77           94.48      642.5          0.09752   \n",
       "556       10.160         19.59           64.73      311.7          0.10030   \n",
       "292       12.950         16.02           83.14      513.7          0.10050   \n",
       "154       13.150         15.34           85.31      538.9          0.09384   \n",
       "377       13.460         28.21           85.89      562.1          0.07517   \n",
       "200       12.230         19.56           78.54      461.0          0.09586   \n",
       "446       17.750         28.03          117.30      981.6          0.09997   \n",
       "501       13.820         24.49           92.33      595.9          0.11620   \n",
       "521       24.630         21.60          165.50     1841.0          0.10300   \n",
       "498       18.490         17.52          121.30     1068.0          0.10120   \n",
       "82        25.220         24.91          171.50     1878.0          0.10630   \n",
       "109       11.340         21.26           72.48      396.5          0.08759   \n",
       "514       15.050         19.07           97.26      701.9          0.09215   \n",
       "13        15.850         23.95          103.70      782.7          0.08401   \n",
       "3         11.420         20.38           77.58      386.1          0.14250   \n",
       "240       13.640         15.60           87.38      575.3          0.09423   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "536       14.270         22.55           93.77      629.8          0.10380   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "513       14.580         13.66           94.29      658.8          0.09832   \n",
       "408       17.990         20.66          117.80      991.7          0.10360   \n",
       "561       11.200         29.37           70.67      386.0          0.07449   \n",
       "118       15.780         22.91          105.70      782.6          0.11550   \n",
       "506       12.220         20.04           79.47      453.1          0.10960   \n",
       "423       13.660         19.13           89.46      575.3          0.09057   \n",
       "222       10.180         17.53           65.12      313.1          0.10610   \n",
       "138       14.950         17.57           96.85      678.1          0.11670   \n",
       "145       11.900         14.65           78.11      432.8          0.11520   \n",
       "148       14.440         15.18           93.97      640.1          0.09970   \n",
       "270       14.290         16.82           90.30      632.6          0.06429   \n",
       "295       13.770         13.27           88.06      582.7          0.09198   \n",
       "370       16.350         23.29          109.00      840.4          0.09742   \n",
       "216       11.890         18.35           77.32      432.2          0.09363   \n",
       "243       13.750         23.77           88.54      590.0          0.08043   \n",
       "419       11.160         21.41           70.95      380.3          0.10180   \n",
       "566       16.600         28.08          108.30      858.1          0.08455   \n",
       "263       15.610         19.38          100.00      758.6          0.07840   \n",
       "31        11.840         18.70           77.93      440.6          0.11090   \n",
       "406       16.140         14.86          104.30      800.0          0.09495   \n",
       "217       10.200         17.48           65.05      321.2          0.08054   \n",
       "20        13.080         15.71           85.63      520.0          0.10750   \n",
       "113       10.510         20.19           68.64      334.2          0.11220   \n",
       "0         17.990         10.38          122.80     1001.0          0.11840   \n",
       "431       12.400         17.68           81.47      467.8          0.10540   \n",
       "281       11.740         14.02           74.24      427.3          0.07813   \n",
       "451       19.590         25.00          127.70     1191.0          0.10320   \n",
       "248       10.650         25.22           68.01      347.0          0.09657   \n",
       "517       19.890         20.26          130.50     1214.0          0.10370   \n",
       "2         19.690         21.25          130.00     1203.0          0.10960   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "350           0.03630        0.008306             0.011620         0.1671   \n",
       "259           0.16390        0.175100             0.083990         0.2091   \n",
       "115           0.07849        0.033280             0.020080         0.1688   \n",
       "60            0.08061        0.010840             0.012900         0.2743   \n",
       "275           0.07210        0.059290             0.074040         0.2015   \n",
       "53            0.14850        0.177200             0.106000         0.2092   \n",
       "221           0.11920        0.078600             0.044510         0.1962   \n",
       "284           0.09580        0.111500             0.033900         0.1432   \n",
       "146           0.17000        0.165900             0.074150         0.2678   \n",
       "480           0.07838        0.029160             0.015270         0.1464   \n",
       "283           0.18020        0.194800             0.090520         0.1876   \n",
       "516           0.12480        0.156900             0.094510         0.1860   \n",
       "99            0.11410        0.093880             0.058390         0.1879   \n",
       "556           0.07504        0.005025             0.011160         0.1791   \n",
       "292           0.07943        0.061550             0.033700         0.1730   \n",
       "154           0.08498        0.092930             0.034830         0.1822   \n",
       "377           0.04726        0.012710             0.011170         0.1421   \n",
       "200           0.08087        0.041870             0.041070         0.1979   \n",
       "446           0.13140        0.169800             0.082930         0.1713   \n",
       "501           0.16810        0.135700             0.067590         0.2275   \n",
       "521           0.21060        0.231000             0.147100         0.1991   \n",
       "498           0.13170        0.149100             0.091830         0.1832   \n",
       "82            0.26650        0.333900             0.184500         0.1829   \n",
       "109           0.06575        0.051330             0.018990         0.1487   \n",
       "514           0.08597        0.074860             0.043350         0.1561   \n",
       "13            0.10020        0.099380             0.053640         0.1847   \n",
       "3             0.28390        0.241400             0.105200         0.2597   \n",
       "240           0.06630        0.047050             0.037310         0.1717   \n",
       "548           0.05030        0.023370             0.009615         0.1580   \n",
       "536           0.11540        0.146300             0.061390         0.1926   \n",
       "..                ...             ...                  ...            ...   \n",
       "513           0.08918        0.082220             0.043490         0.1739   \n",
       "408           0.13040        0.120100             0.088240         0.1992   \n",
       "561           0.03558        0.000000             0.000000         0.1060   \n",
       "118           0.17520        0.213300             0.094790         0.2096   \n",
       "506           0.11520        0.081750             0.021660         0.2124   \n",
       "423           0.11470        0.096570             0.048120         0.1848   \n",
       "222           0.08502        0.017680             0.019150         0.1910   \n",
       "138           0.13050        0.153900             0.086240         0.1957   \n",
       "145           0.12960        0.037100             0.030030         0.1995   \n",
       "148           0.10210        0.084870             0.055320         0.1724   \n",
       "270           0.02675        0.007250             0.006250         0.1508   \n",
       "295           0.06221        0.010630             0.019170         0.1592   \n",
       "370           0.14970        0.181100             0.087730         0.2175   \n",
       "216           0.11540        0.066360             0.031420         0.1967   \n",
       "243           0.06807        0.046970             0.023440         0.1773   \n",
       "419           0.05978        0.008955             0.010760         0.1615   \n",
       "566           0.10230        0.092510             0.053020         0.1590   \n",
       "263           0.05616        0.042090             0.028470         0.1547   \n",
       "31            0.15160        0.121800             0.051820         0.2301   \n",
       "406           0.08501        0.055000             0.045280         0.1735   \n",
       "217           0.05907        0.057740             0.010710         0.1964   \n",
       "20            0.12700        0.045680             0.031100         0.1967   \n",
       "113           0.13030        0.064760             0.030680         0.1922   \n",
       "0             0.27760        0.300100             0.147100         0.2419   \n",
       "431           0.13160        0.077410             0.027990         0.1811   \n",
       "281           0.04340        0.022450             0.027630         0.2101   \n",
       "451           0.09871        0.165500             0.090630         0.1663   \n",
       "248           0.07234        0.023790             0.016150         0.1897   \n",
       "517           0.13100        0.141100             0.094310         0.1802   \n",
       "2             0.15990        0.197400             0.127900         0.2069   \n",
       "\n",
       "     fractal_dimension_mean       ...         concave points_se  symmetry_se  \\\n",
       "350                 0.05731       ...                  0.006296     0.022160   \n",
       "259                 0.06650       ...                  0.010220     0.009947   \n",
       "115                 0.06194       ...                  0.007711     0.012780   \n",
       "60                  0.06960       ...                  0.008193     0.041830   \n",
       "275                 0.05875       ...                  0.019100     0.026780   \n",
       "53                  0.06310       ...                  0.009222     0.026740   \n",
       "221                 0.06303       ...                  0.008360     0.018420   \n",
       "284                 0.05935       ...                  0.017740     0.018780   \n",
       "146                 0.07371       ...                  0.018430     0.056280   \n",
       "480                 0.06284       ...                  0.005161     0.014540   \n",
       "283                 0.06684       ...                  0.012090     0.013880   \n",
       "516                 0.05941       ...                  0.010460     0.015590   \n",
       "99                  0.06390       ...                  0.014240     0.014620   \n",
       "556                 0.06331       ...                  0.007082     0.025720   \n",
       "292                 0.06470       ...                  0.011320     0.026250   \n",
       "154                 0.06207       ...                  0.009536     0.027690   \n",
       "377                 0.05763       ...                  0.005179     0.014420   \n",
       "200                 0.06013       ...                  0.011400     0.015030   \n",
       "446                 0.05916       ...                  0.011100     0.012370   \n",
       "501                 0.07237       ...                  0.016160     0.024340   \n",
       "521                 0.06739       ...                  0.015970     0.018790   \n",
       "498                 0.06697       ...                  0.015730     0.016170   \n",
       "82                  0.06782       ...                  0.020300     0.010650   \n",
       "109                 0.06529       ...                  0.006435     0.015680   \n",
       "514                 0.05915       ...                  0.009423     0.011520   \n",
       "13                  0.05338       ...                  0.019920     0.029810   \n",
       "3                   0.09744       ...                  0.018670     0.059630   \n",
       "240                 0.05660       ...                  0.011030     0.018980   \n",
       "548                 0.06235       ...                  0.009615     0.022030   \n",
       "536                 0.05982       ...                  0.012760     0.014510   \n",
       "..                      ...       ...                       ...          ...   \n",
       "513                 0.05640       ...                  0.008648     0.015390   \n",
       "408                 0.06069       ...                  0.014800     0.014140   \n",
       "561                 0.05502       ...                  0.000000     0.019890   \n",
       "118                 0.07331       ...                  0.013900     0.014950   \n",
       "506                 0.06894       ...                  0.009894     0.013200   \n",
       "423                 0.06181       ...                  0.012740     0.015810   \n",
       "222                 0.06908       ...                  0.007624     0.026370   \n",
       "138                 0.06216       ...                  0.028010     0.051680   \n",
       "145                 0.07839       ...                  0.011100     0.031270   \n",
       "148                 0.06081       ...                  0.014930     0.014540   \n",
       "270                 0.05376       ...                  0.003608     0.015360   \n",
       "295                 0.05912       ...                  0.006829     0.021540   \n",
       "370                 0.06218       ...                  0.016560     0.031970   \n",
       "216                 0.06314       ...                  0.017850     0.027930   \n",
       "243                 0.05429       ...                  0.010070     0.025980   \n",
       "419                 0.06144       ...                  0.006159     0.026940   \n",
       "566                 0.05648       ...                  0.015570     0.013180   \n",
       "263                 0.05443       ...                  0.005174     0.010130   \n",
       "31                  0.07799       ...                  0.010440     0.022730   \n",
       "406                 0.05875       ...                  0.008747     0.015000   \n",
       "217                 0.06315       ...                  0.010710     0.025600   \n",
       "20                  0.06811       ...                  0.006490     0.016780   \n",
       "113                 0.07782       ...                  0.015440     0.022870   \n",
       "0                   0.07871       ...                  0.015870     0.030030   \n",
       "431                 0.07102       ...                  0.011670     0.021870   \n",
       "281                 0.06113       ...                  0.018460     0.029210   \n",
       "451                 0.05391       ...                  0.014990     0.016410   \n",
       "248                 0.06329       ...                  0.006245     0.021580   \n",
       "517                 0.06188       ...                  0.011780     0.010570   \n",
       "2                   0.05999       ...                  0.020580     0.022500   \n",
       "\n",
       "     fractal_dimension_se  radius_worst  texture_worst  perimeter_worst  \\\n",
       "350              0.002668         13.28          19.74            83.61   \n",
       "259              0.003359         18.49          49.54           126.30   \n",
       "115              0.003856         13.67          26.15            87.54   \n",
       "60               0.005953         11.02          17.45            69.86   \n",
       "275              0.003002         12.40          18.99            79.46   \n",
       "53               0.005126         20.60          24.13           135.10   \n",
       "221              0.002918         14.98          17.13           101.10   \n",
       "284              0.003696         13.90          19.69            92.12   \n",
       "146              0.004635         13.74          26.38            91.93   \n",
       "480              0.001858         13.34          27.87            88.83   \n",
       "283              0.004081         18.55          25.09           126.90   \n",
       "516              0.002725         21.86          26.20           142.20   \n",
       "99               0.004452         16.33          30.86           109.50   \n",
       "556              0.002278         10.65          22.88            67.88   \n",
       "292              0.004726         13.74          19.93            88.81   \n",
       "154              0.003479         14.77          20.50            97.67   \n",
       "377              0.001684         14.69          35.63            97.11   \n",
       "200              0.003338         14.44          28.36            92.15   \n",
       "446              0.002556         21.53          38.54           145.40   \n",
       "501              0.006995         16.01          32.94           106.00   \n",
       "521              0.004760         29.92          26.93           205.70   \n",
       "498              0.005255         22.75          22.88           146.40   \n",
       "82               0.005893         30.00          33.62           211.70   \n",
       "109              0.002477         13.01          29.15            83.99   \n",
       "514              0.001718         17.58          28.06           113.80   \n",
       "13               0.003002         16.84          27.66           112.00   \n",
       "3                0.009208         14.91          26.50            98.87   \n",
       "240              0.001794         14.85          19.05            94.11   \n",
       "548              0.004154         10.93          25.59            69.10   \n",
       "536              0.003756         15.29          34.27           104.30   \n",
       "..                    ...           ...            ...              ...   \n",
       "513              0.002281         16.76          17.24           108.50   \n",
       "408              0.003336         21.08          25.41           138.10   \n",
       "561              0.001773         11.92          38.30            75.19   \n",
       "118              0.005984         20.19          30.50           130.30   \n",
       "506              0.003813         13.16          24.17            85.13   \n",
       "423              0.003956         15.14          25.50           101.40   \n",
       "222              0.003761         11.17          22.84            71.94   \n",
       "138              0.002887         18.55          21.43           121.40   \n",
       "145              0.009423         13.15          16.51            86.26   \n",
       "148              0.002528         15.85          19.85           108.60   \n",
       "270              0.001381         14.91          20.65            94.44   \n",
       "295              0.001802         14.67          16.93            94.17   \n",
       "370              0.004085         19.38          31.03           129.30   \n",
       "216              0.004775         13.25          27.10            86.20   \n",
       "243              0.003087         15.01          26.34            98.00   \n",
       "419              0.002060         12.36          28.92            79.26   \n",
       "566              0.003892         18.98          34.12           126.70   \n",
       "263              0.001345         17.91          31.67           115.90   \n",
       "31               0.005667         16.82          28.12           119.40   \n",
       "406              0.001621         17.71          19.58           115.90   \n",
       "217              0.004613         11.48          24.47            75.40   \n",
       "20               0.002425         14.50          20.49            96.09   \n",
       "113              0.006792         11.16          22.75            72.62   \n",
       "0                0.006193         25.38          17.33           184.60   \n",
       "431              0.006005         12.88          22.91            89.61   \n",
       "281              0.002005         13.31          18.26            84.70   \n",
       "451              0.001807         21.44          30.96           139.80   \n",
       "248              0.002619         12.25          35.19            77.98   \n",
       "517              0.003391         23.73          25.23           160.50   \n",
       "2                0.004571         23.57          25.53           152.50   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \n",
       "350       542.5           0.09958            0.06476          0.03046  \n",
       "259      1035.0           0.18830            0.55640          0.57030  \n",
       "115       583.0           0.15000            0.23990          0.15030  \n",
       "60        368.6           0.12750            0.09866          0.02168  \n",
       "275       472.4           0.13590            0.08368          0.07153  \n",
       "53       1321.0           0.12800            0.22970          0.26230  \n",
       "221       686.6           0.13760            0.26980          0.25770  \n",
       "284       595.6           0.09926            0.23170          0.33440  \n",
       "146       591.7           0.13850            0.40920          0.45040  \n",
       "480       547.4           0.12080            0.22790          0.16200  \n",
       "283      1031.0           0.13650            0.47060          0.50260  \n",
       "516      1493.0           0.14920            0.25360          0.37590  \n",
       "99        826.4           0.14310            0.30260          0.31940  \n",
       "556       347.3           0.12650            0.12000          0.01005  \n",
       "292       585.4           0.14830            0.20680          0.22410  \n",
       "154       677.3           0.14780            0.22560          0.30090  \n",
       "377       680.6           0.11080            0.14570          0.07934  \n",
       "200       638.4           0.14290            0.20420          0.13770  \n",
       "446      1437.0           0.14010            0.37620          0.63990  \n",
       "501       788.0           0.17940            0.39660          0.33810  \n",
       "521      2642.0           0.13420            0.41880          0.46580  \n",
       "498      1600.0           0.14120            0.30890          0.35330  \n",
       "82       2562.0           0.15730            0.60760          0.64760  \n",
       "109       518.1           0.16990            0.21960          0.31200  \n",
       "514       967.0           0.12460            0.21010          0.28660  \n",
       "13        876.5           0.11310            0.19240          0.23220  \n",
       "3         567.7           0.20980            0.86630          0.68690  \n",
       "240       683.4           0.12780            0.12910          0.15330  \n",
       "548       364.2           0.11990            0.09546          0.09350  \n",
       "536       728.3           0.13800            0.27330          0.42340  \n",
       "..          ...               ...                ...              ...  \n",
       "513       862.0           0.12230            0.19280          0.24920  \n",
       "408      1349.0           0.14820            0.37350          0.33010  \n",
       "561       439.6           0.09267            0.05494          0.00000  \n",
       "118      1272.0           0.18550            0.49250          0.73560  \n",
       "506       515.3           0.14020            0.23150          0.35350  \n",
       "423       708.8           0.11470            0.31670          0.36600  \n",
       "222       375.6           0.14060            0.14400          0.06572  \n",
       "138       971.4           0.14110            0.21640          0.33550  \n",
       "145       509.6           0.14240            0.25170          0.09420  \n",
       "148       766.9           0.13160            0.27350          0.31030  \n",
       "270       684.6           0.08567            0.05036          0.03866  \n",
       "295       661.1           0.11700            0.10720          0.03732  \n",
       "370      1165.0           0.14150            0.46650          0.70870  \n",
       "216       531.2           0.14050            0.30460          0.28060  \n",
       "243       706.0           0.09368            0.14420          0.13590  \n",
       "419       458.0           0.12820            0.11080          0.03582  \n",
       "566      1124.0           0.11390            0.30940          0.34030  \n",
       "263       988.6           0.10840            0.18070          0.22600  \n",
       "31        888.7           0.16370            0.57750          0.69560  \n",
       "406       947.9           0.12060            0.17220          0.23100  \n",
       "217       403.7           0.09527            0.13970          0.19250  \n",
       "20        630.5           0.13120            0.27760          0.18900  \n",
       "113       374.4           0.13000            0.20490          0.12950  \n",
       "0        2019.0           0.16220            0.66560          0.71190  \n",
       "431       515.8           0.14500            0.26290          0.24030  \n",
       "281       533.7           0.10360            0.08500          0.06735  \n",
       "451      1421.0           0.15280            0.18450          0.39770  \n",
       "248       455.7           0.14990            0.13980          0.11250  \n",
       "517      1646.0           0.14170            0.33090          0.41850  \n",
       "2        1709.0           0.14440            0.42450          0.45040  \n",
       "\n",
       "[171 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.21</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.01724</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>14.37</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.1072</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>13.05</td>\n",
       "      <td>19.31</td>\n",
       "      <td>82.61</td>\n",
       "      <td>527.2</td>\n",
       "      <td>0.08060</td>\n",
       "      <td>0.03789</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.1819</td>\n",
       "      <td>0.05501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.02190</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>14.23</td>\n",
       "      <td>22.25</td>\n",
       "      <td>90.24</td>\n",
       "      <td>624.1</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>0.001845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>12.87</td>\n",
       "      <td>19.54</td>\n",
       "      <td>82.67</td>\n",
       "      <td>509.2</td>\n",
       "      <td>0.09136</td>\n",
       "      <td>0.07883</td>\n",
       "      <td>0.017970</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.06347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>0.02223</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>14.45</td>\n",
       "      <td>24.38</td>\n",
       "      <td>95.14</td>\n",
       "      <td>626.9</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.16520</td>\n",
       "      <td>0.071270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>14.53</td>\n",
       "      <td>13.98</td>\n",
       "      <td>93.86</td>\n",
       "      <td>644.2</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.09242</td>\n",
       "      <td>0.068950</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.06121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011360</td>\n",
       "      <td>0.02207</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>15.80</td>\n",
       "      <td>16.93</td>\n",
       "      <td>103.10</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.14780</td>\n",
       "      <td>0.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018850</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>12.21</td>\n",
       "      <td>14.09</td>\n",
       "      <td>78.78</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.08108</td>\n",
       "      <td>0.07823</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>0.06154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.01921</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>13.13</td>\n",
       "      <td>19.29</td>\n",
       "      <td>87.65</td>\n",
       "      <td>529.9</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.24310</td>\n",
       "      <td>0.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>12.31</td>\n",
       "      <td>16.52</td>\n",
       "      <td>79.19</td>\n",
       "      <td>470.9</td>\n",
       "      <td>0.09172</td>\n",
       "      <td>0.06829</td>\n",
       "      <td>0.033720</td>\n",
       "      <td>0.022720</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.05914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.01386</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>14.11</td>\n",
       "      <td>23.21</td>\n",
       "      <td>89.71</td>\n",
       "      <td>611.1</td>\n",
       "      <td>0.1176</td>\n",
       "      <td>0.18430</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>17.19</td>\n",
       "      <td>22.07</td>\n",
       "      <td>111.60</td>\n",
       "      <td>928.3</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.08995</td>\n",
       "      <td>0.090610</td>\n",
       "      <td>0.065270</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009875</td>\n",
       "      <td>0.01144</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>21.58</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.50</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.1558</td>\n",
       "      <td>0.25670</td>\n",
       "      <td>0.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>11.74</td>\n",
       "      <td>14.69</td>\n",
       "      <td>76.31</td>\n",
       "      <td>426.0</td>\n",
       "      <td>0.08099</td>\n",
       "      <td>0.09661</td>\n",
       "      <td>0.067260</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>0.06758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015280</td>\n",
       "      <td>0.02260</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>12.45</td>\n",
       "      <td>17.60</td>\n",
       "      <td>81.25</td>\n",
       "      <td>473.8</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.27930</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>13.68</td>\n",
       "      <td>16.33</td>\n",
       "      <td>87.76</td>\n",
       "      <td>575.5</td>\n",
       "      <td>0.09277</td>\n",
       "      <td>0.07255</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.1631</td>\n",
       "      <td>0.06155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005077</td>\n",
       "      <td>0.01054</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>15.85</td>\n",
       "      <td>20.20</td>\n",
       "      <td>101.60</td>\n",
       "      <td>773.4</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "543        13.21         28.06           84.88      538.4          0.08671   \n",
       "58         13.05         19.31           82.61      527.2          0.08060   \n",
       "436        12.87         19.54           82.67      509.2          0.09136   \n",
       "453        14.53         13.98           93.86      644.2          0.10990   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "386        12.21         14.09           78.78      462.0          0.08108   \n",
       "74         12.31         16.52           79.19      470.9          0.09172   \n",
       "264        17.19         22.07          111.60      928.3          0.09726   \n",
       "510        11.74         14.69           76.31      426.0          0.08099   \n",
       "532        13.68         16.33           87.76      575.5          0.09277   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "543           0.06877        0.029870             0.032750         0.1628   \n",
       "58            0.03789        0.000692             0.004167         0.1819   \n",
       "436           0.07883        0.017970             0.020900         0.1861   \n",
       "453           0.09242        0.068950             0.064950         0.1650   \n",
       "4             0.13280        0.198000             0.104300         0.1809   \n",
       "386           0.07823        0.068390             0.025340         0.1646   \n",
       "74            0.06829        0.033720             0.022720         0.1720   \n",
       "264           0.08995        0.090610             0.065270         0.1867   \n",
       "510           0.09661        0.067260             0.026390         0.1499   \n",
       "532           0.07255        0.017520             0.018800         0.1631   \n",
       "\n",
       "     fractal_dimension_mean       ...         concave points_se  symmetry_se  \\\n",
       "543                 0.05781       ...                  0.009117      0.01724   \n",
       "58                  0.05501       ...                  0.004167      0.02190   \n",
       "436                 0.06347       ...                  0.006502      0.02223   \n",
       "453                 0.06121       ...                  0.011360      0.02207   \n",
       "4                   0.05883       ...                  0.018850      0.01756   \n",
       "386                 0.06154       ...                  0.010870      0.01921   \n",
       "74                  0.05914       ...                  0.007965      0.01386   \n",
       "264                 0.05580       ...                  0.009875      0.01144   \n",
       "510                 0.06758       ...                  0.015280      0.02260   \n",
       "532                 0.06155       ...                  0.005077      0.01054   \n",
       "\n",
       "     fractal_dimension_se  radius_worst  texture_worst  perimeter_worst  \\\n",
       "543              0.001343         14.37          37.17            92.48   \n",
       "58               0.002990         14.23          22.25            90.24   \n",
       "436              0.002378         14.45          24.38            95.14   \n",
       "453              0.003563         15.80          16.93           103.10   \n",
       "4                0.005115         22.54          16.67           152.20   \n",
       "386              0.004622         13.13          19.29            87.65   \n",
       "74               0.002304         14.11          23.21            89.71   \n",
       "264              0.001575         21.58          29.33           140.50   \n",
       "510              0.006822         12.45          17.60            81.25   \n",
       "532              0.001697         15.85          20.20           101.60   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \n",
       "543       629.6            0.1072            0.13810         0.106200  \n",
       "58        624.1            0.1021            0.06191         0.001845  \n",
       "436       626.9            0.1214            0.16520         0.071270  \n",
       "453       749.9            0.1347            0.14780         0.137300  \n",
       "4        1575.0            0.1374            0.20500         0.400000  \n",
       "386       529.9            0.1026            0.24310         0.307600  \n",
       "74        611.1            0.1176            0.18430         0.170300  \n",
       "264      1436.0            0.1558            0.25670         0.388900  \n",
       "510       473.8            0.1073            0.27930         0.269000  \n",
       "532       773.4            0.1264            0.15640         0.120600  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [-2.07308728e-01  9.66241529e-03  1.35833802e-02  5.66048010e-04\n",
      " -1.99267143e+00 -5.30668862e+00  9.65806455e-01  3.95429565e+00\n",
      "  8.10433592e-01  5.58426833e+00  1.53554857e-01 -2.62394603e-02\n",
      " -1.88840380e-02  6.42084918e-04  7.79970515e+00 -7.02596823e-01\n",
      " -4.68122170e+00  1.39662745e+01  5.17055371e+00  7.02418771e+00\n",
      "  2.32777227e-01  2.29762745e-03  1.24914929e-05 -1.31925243e-03\n",
      "  2.45853093e+00  5.09704868e-01  6.75234751e-01]\n",
      "Intercept: \n",
      " -2.0455083789919124\n"
     ]
    }
   ],
   "source": [
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import linear_model\n",
    "# Create linear regression object\n",
    "linear = linear_model.LinearRegression()\n",
    "# Train the model using the training sets and check score\n",
    "linear.fit(X_train, y_train)\n",
    "linear.score(X_train, y_train)\n",
    "#Equation coefficient and Intercept\n",
    "print('Coefficient: \\n', linear.coef_)\n",
    "print('Intercept: \\n', linear.intercept_)\n",
    "#Predict Output\n",
    "predicted = linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "10   0\n",
       "11   0\n",
       "12   0\n",
       "13   0\n",
       "14   0\n",
       "15   0\n",
       "16   0\n",
       "17   0\n",
       "18   1\n",
       "19   0\n",
       "20   1\n",
       "21   0\n",
       "22   1\n",
       "23   0\n",
       "24   0\n",
       "25   0\n",
       "26   1\n",
       "27   0\n",
       "28   0\n",
       "29   0\n",
       "..  ..\n",
       "141  0\n",
       "142  0\n",
       "143  0\n",
       "144  1\n",
       "145  0\n",
       "146  0\n",
       "147  0\n",
       "148  1\n",
       "149  0\n",
       "150  0\n",
       "151  0\n",
       "152  0\n",
       "153  1\n",
       "154  0\n",
       "155  0\n",
       "156  0\n",
       "157  0\n",
       "158  0\n",
       "159  1\n",
       "160  0\n",
       "161  0\n",
       "162  0\n",
       "163  0\n",
       "164  1\n",
       "165  0\n",
       "166  0\n",
       "167  0\n",
       "168  0\n",
       "169  0\n",
       "170  1\n",
       "\n",
       "[171 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [int(i)for i in predicted] #these are the values predicted we take either 0 or 1\n",
    "\n",
    "Z = pd.DataFrame(z)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 171 entries, 350 to 2\n",
      "Data columns (total 27 columns):\n",
      "radius_mean               171 non-null float64\n",
      "texture_mean              171 non-null float64\n",
      "perimeter_mean            171 non-null float64\n",
      "area_mean                 171 non-null float64\n",
      "smoothness_mean           171 non-null float64\n",
      "compactness_mean          171 non-null float64\n",
      "concavity_mean            171 non-null float64\n",
      "concave points_mean       171 non-null float64\n",
      "symmetry_mean             171 non-null float64\n",
      "fractal_dimension_mean    171 non-null float64\n",
      "radius_se                 171 non-null float64\n",
      "texture_se                171 non-null float64\n",
      "perimeter_se              171 non-null float64\n",
      "area_se                   171 non-null float64\n",
      "smoothness_se             171 non-null float64\n",
      "compactness_se            171 non-null float64\n",
      "concavity_se              171 non-null float64\n",
      "concave points_se         171 non-null float64\n",
      "symmetry_se               171 non-null float64\n",
      "fractal_dimension_se      171 non-null float64\n",
      "radius_worst              171 non-null float64\n",
      "texture_worst             171 non-null float64\n",
      "perimeter_worst           171 non-null float64\n",
      "area_worst                171 non-null float64\n",
      "smoothness_worst          171 non-null float64\n",
      "compactness_worst         171 non-null float64\n",
      "concavity_worst           171 non-null float64\n",
      "dtypes: float64(27)\n",
      "memory usage: 37.4 KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8070175438596491"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "y = accuracy_score(y_test,Z)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do i get the values of all the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I use the full dataset of X which is not sliced I can be able to get the values of all the inputs\n",
    "w = linear.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  1\n",
       "3  1\n",
       "4  0\n",
       "5  0\n",
       "6  0\n",
       "7  0\n",
       "8  0\n",
       "9  1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn the values into dataframes\n",
    "w1 = [int(i) for i in w]\n",
    "w2 = pd.DataFrame(w1)\n",
    "w2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>842302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>842517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>84300903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>84348301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>84358402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>843786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>844359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>84458202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>844981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>84501001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Orig Predicted        ID\n",
       "0    M         1    842302\n",
       "1    M         0    842517\n",
       "2    M         1  84300903\n",
       "3    M         1  84348301\n",
       "4    M         0  84358402\n",
       "5    M         0    843786\n",
       "6    M         0    844359\n",
       "7    M         0  84458202\n",
       "8    M         0    844981\n",
       "9    M         1  84501001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i want to add the values to their IDs and the original diagnosis\n",
    "labels = ['Orig','Predicted','ID']\n",
    "l1 = data.iloc[:,1]\n",
    "l2 = w2.iloc[:,0]\n",
    "l3 = data.iloc[:,0]\n",
    "columns = [l1,l2,l3]\n",
    "# for i, j in zip(l1,l2):\n",
    "#     print(i,j)\n",
    "df = pd.DataFrame(columns,labels)\n",
    "df.T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression\n",
    "Don’t get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).\n",
    "\n",
    "Again, let us try and understand this through a simple example.\n",
    "\n",
    "Let’s say your friend gives you a puzzle to solve. There are only 2 outcome scenarios – either you solve it or you don’t. Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. The outcome to this study would be something like this – if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. This is what Logistic Regression provides you.\n",
    "\n",
    "Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.\n",
    "\n",
    "\n",
    "odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence\n",
    "ln(odds) = ln(p/(1-p))\n",
    "logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk\n",
    "\n",
    "\n",
    "Above, p is the probability of presence of the characteristic of interest. It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).\n",
    "\n",
    "Now, you may ask, why take a log? For the sake of simplicity, let’s just say that this is one of the best mathematical way to replicate a step function. I can go in more details, but that will beat the purpose of this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018850</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.01369</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.01486</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>0.02143</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.01460</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.04484</td>\n",
       "      <td>0.012840</td>\n",
       "      <td>20.960</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.02981</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.01961</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.01857</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.01689</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.01356</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>27.320</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>0.01980</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.01678</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.02027</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022520</td>\n",
       "      <td>0.03672</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>0.01083</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>29.170</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.01468</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>26.460</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>0.02308</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>22.250</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013520</td>\n",
       "      <td>0.01454</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019110</td>\n",
       "      <td>0.02293</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>21.310</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010830</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.01925</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.02105</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>8.678</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>0.01840</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>12.260</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>0.02068</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>16.220</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.01870</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.01724</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>14.370</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.01490</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.02087</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.01560</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>11.250</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010970</td>\n",
       "      <td>0.02277</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>10.830</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.02203</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.02466</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>13.030</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03004</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.02912</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>12.020</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009305</td>\n",
       "      <td>0.01897</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.03759</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>0.01695</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017210</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.02572</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>10.650</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03004</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.01638</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012670</td>\n",
       "      <td>0.01488</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.02080</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01989</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.02137</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026240</td>\n",
       "      <td>0.02057</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>24.290</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.01114</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016780</td>\n",
       "      <td>0.01898</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015570</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016640</td>\n",
       "      <td>0.02324</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02676</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0         17.990         10.38          122.80     1001.0          0.11840   \n",
       "1         20.570         17.77          132.90     1326.0          0.08474   \n",
       "2         19.690         21.25          130.00     1203.0          0.10960   \n",
       "3         11.420         20.38           77.58      386.1          0.14250   \n",
       "4         20.290         14.34          135.10     1297.0          0.10030   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "6         18.250         19.98          119.60     1040.0          0.09463   \n",
       "7         13.710         20.83           90.20      577.9          0.11890   \n",
       "8         13.000         21.82           87.50      519.8          0.12730   \n",
       "9         12.460         24.04           83.97      475.9          0.11860   \n",
       "10        16.020         23.24          102.70      797.8          0.08206   \n",
       "11        15.780         17.89          103.60      781.0          0.09710   \n",
       "12        19.170         24.80          132.40     1123.0          0.09740   \n",
       "13        15.850         23.95          103.70      782.7          0.08401   \n",
       "14        13.730         22.61           93.60      578.3          0.11310   \n",
       "15        14.540         27.54           96.73      658.8          0.11390   \n",
       "16        14.680         20.13           94.74      684.5          0.09867   \n",
       "17        16.130         20.68          108.10      798.8          0.11700   \n",
       "18        19.810         22.15          130.00     1260.0          0.09831   \n",
       "19        13.540         14.36           87.46      566.3          0.09779   \n",
       "20        13.080         15.71           85.63      520.0          0.10750   \n",
       "21         9.504         12.44           60.34      273.9          0.10240   \n",
       "22        15.340         14.26          102.50      704.4          0.10730   \n",
       "23        21.160         23.04          137.20     1404.0          0.09428   \n",
       "24        16.650         21.38          110.00      904.6          0.11210   \n",
       "25        17.140         16.40          116.00      912.7          0.11860   \n",
       "26        14.580         21.53           97.41      644.8          0.10540   \n",
       "27        18.610         20.25          122.10     1094.0          0.09440   \n",
       "28        15.300         25.27          102.40      732.4          0.10820   \n",
       "29        17.570         15.05          115.00      955.1          0.09847   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "539        7.691         25.44           48.34      170.4          0.08668   \n",
       "540       11.540         14.44           74.65      402.9          0.09984   \n",
       "541       14.470         24.99           95.81      656.4          0.08837   \n",
       "542       14.740         25.42           94.70      668.6          0.08275   \n",
       "543       13.210         28.06           84.88      538.4          0.08671   \n",
       "544       13.870         20.70           89.77      584.8          0.09578   \n",
       "545       13.620         23.23           87.19      573.2          0.09246   \n",
       "546       10.320         16.35           65.31      324.9          0.09434   \n",
       "547       10.260         16.58           65.85      320.8          0.08877   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "549       10.820         24.21           68.89      361.6          0.08192   \n",
       "550       10.860         21.48           68.51      360.5          0.07431   \n",
       "551       11.130         22.44           71.49      378.4          0.09566   \n",
       "552       12.770         29.43           81.35      507.9          0.08276   \n",
       "553        9.333         21.94           59.01      264.0          0.09240   \n",
       "554       12.880         28.92           82.50      514.3          0.08123   \n",
       "555       10.290         27.61           65.67      321.4          0.09030   \n",
       "556       10.160         19.59           64.73      311.7          0.10030   \n",
       "557        9.423         27.88           59.26      271.3          0.08123   \n",
       "558       14.590         22.68           96.39      657.1          0.08473   \n",
       "559       11.510         23.93           74.52      403.5          0.09261   \n",
       "560       14.050         27.15           91.38      600.4          0.09929   \n",
       "561       11.200         29.37           70.67      386.0          0.07449   \n",
       "562       15.220         30.62          103.40      716.9          0.10480   \n",
       "563       20.920         25.09          143.00     1347.0          0.10990   \n",
       "564       21.560         22.39          142.00     1479.0          0.11100   \n",
       "565       20.130         28.25          131.20     1261.0          0.09780   \n",
       "566       16.600         28.08          108.30      858.1          0.08455   \n",
       "567       20.600         29.33          140.10     1265.0          0.11780   \n",
       "568        7.760         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760        0.300100             0.147100         0.2419   \n",
       "1             0.07864        0.086900             0.070170         0.1812   \n",
       "2             0.15990        0.197400             0.127900         0.2069   \n",
       "3             0.28390        0.241400             0.105200         0.2597   \n",
       "4             0.13280        0.198000             0.104300         0.1809   \n",
       "5             0.17000        0.157800             0.080890         0.2087   \n",
       "6             0.10900        0.112700             0.074000         0.1794   \n",
       "7             0.16450        0.093660             0.059850         0.2196   \n",
       "8             0.19320        0.185900             0.093530         0.2350   \n",
       "9             0.23960        0.227300             0.085430         0.2030   \n",
       "10            0.06669        0.032990             0.033230         0.1528   \n",
       "11            0.12920        0.099540             0.066060         0.1842   \n",
       "12            0.24580        0.206500             0.111800         0.2397   \n",
       "13            0.10020        0.099380             0.053640         0.1847   \n",
       "14            0.22930        0.212800             0.080250         0.2069   \n",
       "15            0.15950        0.163900             0.073640         0.2303   \n",
       "16            0.07200        0.073950             0.052590         0.1586   \n",
       "17            0.20220        0.172200             0.102800         0.2164   \n",
       "18            0.10270        0.147900             0.094980         0.1582   \n",
       "19            0.08129        0.066640             0.047810         0.1885   \n",
       "20            0.12700        0.045680             0.031100         0.1967   \n",
       "21            0.06492        0.029560             0.020760         0.1815   \n",
       "22            0.21350        0.207700             0.097560         0.2521   \n",
       "23            0.10220        0.109700             0.086320         0.1769   \n",
       "24            0.14570        0.152500             0.091700         0.1995   \n",
       "25            0.22760        0.222900             0.140100         0.3040   \n",
       "26            0.18680        0.142500             0.087830         0.2252   \n",
       "27            0.10660        0.149000             0.077310         0.1697   \n",
       "28            0.16970        0.168300             0.087510         0.1926   \n",
       "29            0.11570        0.098750             0.079530         0.1739   \n",
       "..                ...             ...                  ...            ...   \n",
       "539           0.11990        0.092520             0.013640         0.2037   \n",
       "540           0.11200        0.067370             0.025940         0.1818   \n",
       "541           0.12300        0.100900             0.038900         0.1872   \n",
       "542           0.07214        0.041050             0.030270         0.1840   \n",
       "543           0.06877        0.029870             0.032750         0.1628   \n",
       "544           0.10180        0.036880             0.023690         0.1620   \n",
       "545           0.06747        0.029740             0.024430         0.1664   \n",
       "546           0.04994        0.010120             0.005495         0.1885   \n",
       "547           0.08066        0.043580             0.024380         0.1669   \n",
       "548           0.05030        0.023370             0.009615         0.1580   \n",
       "549           0.06602        0.015480             0.008160         0.1976   \n",
       "550           0.04227        0.000000             0.000000         0.1661   \n",
       "551           0.08194        0.048240             0.022570         0.2030   \n",
       "552           0.04234        0.019970             0.014990         0.1539   \n",
       "553           0.05605        0.039960             0.012820         0.1692   \n",
       "554           0.05824        0.061950             0.023430         0.1566   \n",
       "555           0.07658        0.059990             0.027380         0.1593   \n",
       "556           0.07504        0.005025             0.011160         0.1791   \n",
       "557           0.04971        0.000000             0.000000         0.1742   \n",
       "558           0.13300        0.102900             0.037360         0.1454   \n",
       "559           0.10210        0.111200             0.041050         0.1388   \n",
       "560           0.11260        0.044620             0.043040         0.1537   \n",
       "561           0.03558        0.000000             0.000000         0.1060   \n",
       "562           0.20870        0.255000             0.094290         0.2128   \n",
       "563           0.22360        0.317400             0.147400         0.2149   \n",
       "564           0.11590        0.243900             0.138900         0.1726   \n",
       "565           0.10340        0.144000             0.097910         0.1752   \n",
       "566           0.10230        0.092510             0.053020         0.1590   \n",
       "567           0.27700        0.351400             0.152000         0.2397   \n",
       "568           0.04362        0.000000             0.000000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean       ...         concave points_se  symmetry_se  \\\n",
       "0                   0.07871       ...                  0.015870      0.03003   \n",
       "1                   0.05667       ...                  0.013400      0.01389   \n",
       "2                   0.05999       ...                  0.020580      0.02250   \n",
       "3                   0.09744       ...                  0.018670      0.05963   \n",
       "4                   0.05883       ...                  0.018850      0.01756   \n",
       "5                   0.07613       ...                  0.011370      0.02165   \n",
       "6                   0.05742       ...                  0.010390      0.01369   \n",
       "7                   0.07451       ...                  0.014480      0.01486   \n",
       "8                   0.07389       ...                  0.012260      0.02143   \n",
       "9                   0.08243       ...                  0.014320      0.01789   \n",
       "10                  0.05697       ...                  0.007591      0.01460   \n",
       "11                  0.06082       ...                  0.012820      0.02008   \n",
       "12                  0.07800       ...                  0.040900      0.04484   \n",
       "13                  0.05338       ...                  0.019920      0.02981   \n",
       "14                  0.07682       ...                  0.016280      0.01961   \n",
       "15                  0.07077       ...                  0.010900      0.01857   \n",
       "16                  0.05922       ...                  0.011090      0.01410   \n",
       "17                  0.07356       ...                  0.012970      0.01689   \n",
       "18                  0.05395       ...                  0.015210      0.01356   \n",
       "19                  0.05766       ...                  0.013150      0.01980   \n",
       "20                  0.06811       ...                  0.006490      0.01678   \n",
       "21                  0.06905       ...                  0.014210      0.02027   \n",
       "22                  0.07032       ...                  0.022520      0.03672   \n",
       "23                  0.05278       ...                  0.010380      0.01083   \n",
       "24                  0.06330       ...                  0.011300      0.01468   \n",
       "25                  0.07413       ...                  0.023970      0.02308   \n",
       "26                  0.06924       ...                  0.013520      0.01454   \n",
       "27                  0.05699       ...                  0.019110      0.02293   \n",
       "28                  0.06540       ...                  0.010830      0.01768   \n",
       "29                  0.06149       ...                  0.013540      0.01925   \n",
       "..                      ...       ...                       ...          ...   \n",
       "539                 0.07751       ...                  0.013640      0.02105   \n",
       "540                 0.06782       ...                  0.014940      0.01840   \n",
       "541                 0.06341       ...                  0.011620      0.02068   \n",
       "542                 0.05680       ...                  0.012690      0.01870   \n",
       "543                 0.05781       ...                  0.009117      0.01724   \n",
       "544                 0.06688       ...                  0.009061      0.01490   \n",
       "545                 0.05801       ...                  0.009064      0.02087   \n",
       "546                 0.06201       ...                  0.005495      0.01560   \n",
       "547                 0.06714       ...                  0.010970      0.02277   \n",
       "548                 0.06235       ...                  0.009615      0.02203   \n",
       "549                 0.06328       ...                  0.005917      0.02466   \n",
       "550                 0.05948       ...                  0.000000      0.03004   \n",
       "551                 0.06552       ...                  0.010240      0.02912   \n",
       "552                 0.05637       ...                  0.009305      0.01897   \n",
       "553                 0.06576       ...                  0.012820      0.03759   \n",
       "554                 0.05708       ...                  0.007620      0.01695   \n",
       "555                 0.06127       ...                  0.017210      0.01843   \n",
       "556                 0.06331       ...                  0.007082      0.02572   \n",
       "557                 0.06059       ...                  0.000000      0.03004   \n",
       "558                 0.06147       ...                  0.016060      0.01638   \n",
       "559                 0.06570       ...                  0.012670      0.01488   \n",
       "560                 0.06171       ...                  0.016260      0.02080   \n",
       "561                 0.05502       ...                  0.000000      0.01989   \n",
       "562                 0.07152       ...                  0.016080      0.02137   \n",
       "563                 0.06879       ...                  0.026240      0.02057   \n",
       "564                 0.05623       ...                  0.024540      0.01114   \n",
       "565                 0.05533       ...                  0.016780      0.01898   \n",
       "566                 0.05648       ...                  0.015570      0.01318   \n",
       "567                 0.07016       ...                  0.016640      0.02324   \n",
       "568                 0.05884       ...                  0.000000      0.02676   \n",
       "\n",
       "     fractal_dimension_se  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                0.006193        25.380          17.33           184.60   \n",
       "1                0.003532        24.990          23.41           158.80   \n",
       "2                0.004571        23.570          25.53           152.50   \n",
       "3                0.009208        14.910          26.50            98.87   \n",
       "4                0.005115        22.540          16.67           152.20   \n",
       "5                0.005082        15.470          23.75           103.40   \n",
       "6                0.002179        22.880          27.66           153.20   \n",
       "7                0.005412        17.060          28.14           110.60   \n",
       "8                0.003749        15.490          30.73           106.20   \n",
       "9                0.010080        15.090          40.68            97.65   \n",
       "10               0.003042        19.190          33.88           123.80   \n",
       "11               0.004144        20.420          27.28           136.50   \n",
       "12               0.012840        20.960          29.94           151.70   \n",
       "13               0.003002        16.840          27.66           112.00   \n",
       "14               0.008093        15.030          32.01           108.80   \n",
       "15               0.005466        17.460          37.13           124.10   \n",
       "16               0.002085        19.070          30.88           123.40   \n",
       "17               0.004142        20.960          31.48           136.80   \n",
       "18               0.001997        27.320          30.88           186.80   \n",
       "19               0.002300        15.110          19.26            99.70   \n",
       "20               0.002425        14.500          20.49            96.09   \n",
       "21               0.002968        10.230          15.66            65.13   \n",
       "22               0.004394        18.070          19.08           125.10   \n",
       "23               0.001987        29.170          35.59           188.00   \n",
       "24               0.002801        26.460          31.56           177.00   \n",
       "25               0.007444        22.250          21.40           152.40   \n",
       "26               0.003711        17.620          33.21           122.40   \n",
       "27               0.004217        21.310          27.26           139.90   \n",
       "28               0.002967        20.270          36.71           149.30   \n",
       "29               0.003742        20.010          19.52           134.90   \n",
       "..                    ...           ...            ...              ...   \n",
       "539              0.007551         8.678          31.89            54.49   \n",
       "540              0.005512        12.260          19.68            78.78   \n",
       "541              0.006111        16.220          31.73           113.50   \n",
       "542              0.002626        16.510          32.29           107.40   \n",
       "543              0.001343        14.370          37.17            92.48   \n",
       "544              0.003599        15.050          24.75            99.17   \n",
       "545              0.002583        15.350          29.09            97.58   \n",
       "546              0.002606        11.250          21.77            71.12   \n",
       "547              0.005890        10.830          22.04            71.08   \n",
       "548              0.004154        10.930          25.59            69.10   \n",
       "549              0.002977        13.030          31.45            83.90   \n",
       "550              0.002228        11.660          24.77            74.08   \n",
       "551              0.004723        12.020          28.26            77.80   \n",
       "552              0.001726        13.870          36.00            88.10   \n",
       "553              0.004623         9.845          25.05            62.86   \n",
       "554              0.002801        13.890          35.74            88.84   \n",
       "555              0.004938        10.840          34.91            69.57   \n",
       "556              0.002278        10.650          22.88            67.88   \n",
       "557              0.003324        10.490          34.24            66.50   \n",
       "558              0.004406        15.480          27.27           105.90   \n",
       "559              0.004738        12.480          37.16            82.28   \n",
       "560              0.005304        15.300          33.17           100.20   \n",
       "561              0.001773        11.920          38.30            75.19   \n",
       "562              0.006142        17.520          42.79           128.70   \n",
       "563              0.006213        24.290          29.41           179.10   \n",
       "564              0.004239        25.450          26.40           166.10   \n",
       "565              0.002498        23.690          38.25           155.00   \n",
       "566              0.003892        18.980          34.12           126.70   \n",
       "567              0.006185        25.740          39.42           184.60   \n",
       "568              0.002783         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \n",
       "0        2019.0           0.16220            0.66560          0.71190  \n",
       "1        1956.0           0.12380            0.18660          0.24160  \n",
       "2        1709.0           0.14440            0.42450          0.45040  \n",
       "3         567.7           0.20980            0.86630          0.68690  \n",
       "4        1575.0           0.13740            0.20500          0.40000  \n",
       "5         741.6           0.17910            0.52490          0.53550  \n",
       "6        1606.0           0.14420            0.25760          0.37840  \n",
       "7         897.0           0.16540            0.36820          0.26780  \n",
       "8         739.3           0.17030            0.54010          0.53900  \n",
       "9         711.4           0.18530            1.05800          1.10500  \n",
       "10       1150.0           0.11810            0.15510          0.14590  \n",
       "11       1299.0           0.13960            0.56090          0.39650  \n",
       "12       1332.0           0.10370            0.39030          0.36390  \n",
       "13        876.5           0.11310            0.19240          0.23220  \n",
       "14        697.7           0.16510            0.77250          0.69430  \n",
       "15        943.2           0.16780            0.65770          0.70260  \n",
       "16       1138.0           0.14640            0.18710          0.29140  \n",
       "17       1315.0           0.17890            0.42330          0.47840  \n",
       "18       2398.0           0.15120            0.31500          0.53720  \n",
       "19        711.2           0.14400            0.17730          0.23900  \n",
       "20        630.5           0.13120            0.27760          0.18900  \n",
       "21        314.9           0.13240            0.11480          0.08867  \n",
       "22        980.9           0.13900            0.59540          0.63050  \n",
       "23       2615.0           0.14010            0.26000          0.31550  \n",
       "24       2215.0           0.18050            0.35780          0.46950  \n",
       "25       1461.0           0.15450            0.39490          0.38530  \n",
       "26        896.9           0.15250            0.66430          0.55390  \n",
       "27       1403.0           0.13380            0.21170          0.34460  \n",
       "28       1269.0           0.16410            0.61100          0.63350  \n",
       "29       1227.0           0.12550            0.28120          0.24890  \n",
       "..          ...               ...                ...              ...  \n",
       "539       223.6           0.15960            0.30640          0.33930  \n",
       "540       457.8           0.13450            0.21180          0.17970  \n",
       "541       808.9           0.13400            0.42020          0.40400  \n",
       "542       826.4           0.10600            0.13760          0.16110  \n",
       "543       629.6           0.10720            0.13810          0.10620  \n",
       "544       688.6           0.12640            0.20370          0.13770  \n",
       "545       729.8           0.12160            0.15170          0.10490  \n",
       "546       384.9           0.12850            0.08842          0.04384  \n",
       "547       357.4           0.14610            0.22460          0.17830  \n",
       "548       364.2           0.11990            0.09546          0.09350  \n",
       "549       505.6           0.12040            0.16330          0.06194  \n",
       "550       412.3           0.10010            0.07348          0.00000  \n",
       "551       436.6           0.10870            0.17820          0.15640  \n",
       "552       594.7           0.12340            0.10640          0.08653  \n",
       "553       295.8           0.11030            0.08298          0.07993  \n",
       "554       595.7           0.12270            0.16200          0.24390  \n",
       "555       357.6           0.13840            0.17100          0.20000  \n",
       "556       347.3           0.12650            0.12000          0.01005  \n",
       "557       330.6           0.10730            0.07158          0.00000  \n",
       "558       733.5           0.10260            0.31710          0.36620  \n",
       "559       474.2           0.12980            0.25170          0.36300  \n",
       "560       706.7           0.12410            0.22640          0.13260  \n",
       "561       439.6           0.09267            0.05494          0.00000  \n",
       "562       915.0           0.14170            0.79170          1.17000  \n",
       "563      1819.0           0.14070            0.41860          0.65990  \n",
       "564      2027.0           0.14100            0.21130          0.41070  \n",
       "565      1731.0           0.11660            0.19220          0.32150  \n",
       "566      1124.0           0.11390            0.30940          0.34030  \n",
       "567      1821.0           0.16500            0.86810          0.93870  \n",
       "568       268.6           0.08996            0.06444          0.00000  \n",
       "\n",
       "[569 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = X.copy() #this is fine for our tex\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = data.iloc[:,1]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y2 = le.fit_transform(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X2_train,X2_test,y2_train,y2_test, = train_test_split(X2,y2,test_size = 0.25, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[-2.11342248e+00 -1.20723647e-01  6.71357864e-02  2.46808939e-03\n",
      "   1.52495081e-01  4.07565401e-01  6.45674995e-01  3.37782817e-01\n",
      "   2.27634878e-01  2.67091529e-02  2.00587323e-02 -1.23058948e+00\n",
      "  -4.63832723e-02  9.68119889e-02  1.65755128e-02  1.21701405e-03\n",
      "   5.24547534e-02  4.00069633e-02  4.40252015e-02 -5.48599772e-03\n",
      "  -1.28854764e+00  3.43611451e-01  1.32890025e-01  2.43158691e-02\n",
      "   2.82076100e-01  1.16375973e+00  1.59740370e+00]]\n",
      "Intercept: \n",
      " [-0.39424138]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create logistic regression object\n",
    "model = LogisticRegression()\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X2, y2)\n",
    "model.score(X2, y2)\n",
    "#Equation coefficient and Intercept\n",
    "print('Coefficient: \\n', model.coef_)\n",
    "print('Intercept: \\n', model.intercept_)\n",
    "#Predict Output\n",
    "predicted2 = model.predict(X2_test)\n",
    "predicted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y2_test,predicted2)#calculate the accuracy score, and its better than logistics regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97  1]\n",
      " [ 5 40]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y2_test,predicted2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97        98\n",
      "          1       0.98      0.89      0.93        45\n",
      "\n",
      "avg / total       0.96      0.96      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y2_test,predicted2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Furthermore..\n",
    "There are many different steps that could be tried in order to improve the model:\n",
    "\n",
    "* including interaction terms\n",
    "* removing features\n",
    "* regularization techniques\n",
    "* using a non-linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Tree\n",
    "This is one of my favorite algorithm and I use it quite frequently. It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. For more details, you can read: Decision Tree Simplified.\n",
    "\n",
    "Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based methods empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).\n",
    "\n",
    "Methods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. Hence, for every analyst (fresher also), it’s important to learn these algorithms and use them for modeling.\n",
    "\n",
    "#### Example:-\n",
    "\n",
    "Let’s say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket in leisure time. Now, I want to create a model to predict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.\n",
    "\n",
    "This is where decision tree helps, it will segregate the students based on all values of three variable and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.\n",
    "\n",
    "Types of Decision Trees\n",
    "Types of decision tree is based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "* Categorical Variable Decision Tree: Decision Tree which has categorical target variable then it called as categorical variable decision tree. Example:- In above scenario of student problem, where the target variable was “Student will play cricket or not” i.e. YES or NO.\n",
    "\n",
    "* Continuous Variable Decision Tree: Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.\n",
    "\n",
    "Example:- Let’s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (yes/ no). Here we know that income of customer is a significant variable but insurance company does not have income details for all customers. Now, as we know this is an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable.\n",
    "\n",
    "### Important Terminology related to Decision Trees\n",
    "Let’s look at the basic terminology used with Decision trees:\n",
    "\n",
    "* Root Node: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "* Splitting: It is a process of dividing a node into two or more sub-nodes.\n",
    "* Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "* Leaf/ Terminal Node: Nodes do not split is called Leaf or Terminal node\n",
    "* Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "* Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree.\n",
    "* Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "\n",
    "## Advantages\n",
    "* Easy to Understand: Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "* Useful in Data exploration: Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. With the help of decision trees, we can create new variables / features that has better power to predict target variable. You can refer article (Trick to enhance power of regression model) for one such trick.  It can also be used in data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.\n",
    "* Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.\n",
    "* Data type is not a constraint: It can handle both numerical and categorical variables.\n",
    "* Non Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.\n",
    " \n",
    "\n",
    "## Disadvantages\n",
    "* Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below).\n",
    "* Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = X2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 27), (143, 27), (426,), (143,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3,y3, test_size = 0.25,random_state = 10)\n",
    "X3_train.shape, X3_test.shape, y3_train.shape, y3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X3, y3)\n",
    "model.score(X3, y3)\n",
    "#Predict Output\n",
    "predicted3= model.predict(X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y3_test, predicted3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM (Support Vector Machine)\n",
    "It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.\n",
    "\n",
    "For example, if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as Support Vectors)\n",
    "\n",
    "## What is a classification analysis?\n",
    "Let’s consider an example to understand these concepts. We have a population composed of 50%-50% Males and Females. Using a sample of this population, you want to create some set of rules which will guide us the gender class for rest of the population. Using this algorithm, we intend to build a robot which can identify whether a person is a Male or a Female. This is a sample problem of classification analysis. Using some set of rules, we will try to classify the population into two possible segments. For simplicity, let’s assume that the two differentiating factors identified are : Height of the individual and Hair Length. Following is a scatter plot of the sample.\n",
    "\n",
    "<img src=\"xyplot.webp\">\n",
    "\n",
    "The blue circles in the plot represent females and green squares represents male. A few expected insights from the graph are :\n",
    "\n",
    "1. Males in our population have a higher average height.\n",
    "\n",
    "2. Females in our population have longer scalp hairs.\n",
    "\n",
    "If we were to see an individual with height 180 cms and hair length 4 cms, our best guess will be to classify this individual as a male. This is how we do a classification analysis.\n",
    "\n",
    "## What is a Support Vector and what is SVM?\n",
    "Support Vectors are simply the co-ordinates of individual observation. For instance, (45,150) is a support vector which corresponds to a female. Support Vector Machine is a frontier which best segregates the Male from the Females. In this case, the two classes are well separated from each other, hence it is easier to find a SVM.\n",
    "\n",
    "### How to find the Support Vector Machine for case in hand?\n",
    "There are many possible frontier which can classify the problem in hand. Following are the three possible frontiers.\n",
    "\n",
    "<img src=\"xyplot1.webp\">\n",
    "\n",
    "How do we decide which is the best frontier for this particular problem statement?\n",
    "\n",
    "The easiest way to interpret the objective function in a SVM is to find the minimum distance of the frontier from closest support vector (this can belong to any class). For instance, orange frontier is closest to blue circles. And the closest blue circle is 2 units away from the frontier. Once we have these distances for all the frontiers, we simply choose the frontier with the maximum distance (from the closest support vector). Out of the three shown frontiers, we see the black frontier is farthest from nearest support vector (i.e. 15 units).\n",
    "\n",
    "### What if we do not find a clean frontier which segregates the classes?\n",
    "Our job was relatively easier finding the SVM in this business case. What if the distribution looked something like as follows :\n",
    "\n",
    "<img src=\"xyplot2.webp\">\n",
    "In such cases, we do not see a straight line frontier directly in current plane which can serve as the SVM. In such cases, we need to map these vector to a higher dimension plane so that they get segregated from each other. Such cases will be covered once we start with the formulation of SVM. For now, you can visualize that such transformation will result into following type of SVM.\n",
    "\n",
    "<img src=\"xyplot3.webp\">\n",
    "\n",
    "Each of the green square in original distribution is mapped on a transformed scale. And transformed scale has clearly segregated classes. Many algorithms have been proposed to make these transformations and some of which will be discussed in following articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01137</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01039</td>\n",
       "      <td>0.01369</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01448</td>\n",
       "      <td>0.01486</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01226</td>\n",
       "      <td>0.02143</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01432</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "5        12.45         15.70           82.57      477.1          0.12780   \n",
       "6        18.25         19.98          119.60     1040.0          0.09463   \n",
       "7        13.71         20.83           90.20      577.9          0.11890   \n",
       "8        13.00         21.82           87.50      519.8          0.12730   \n",
       "9        12.46         24.04           83.97      475.9          0.11860   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760         0.30010              0.14710         0.2419   \n",
       "1           0.07864         0.08690              0.07017         0.1812   \n",
       "2           0.15990         0.19740              0.12790         0.2069   \n",
       "3           0.28390         0.24140              0.10520         0.2597   \n",
       "4           0.13280         0.19800              0.10430         0.1809   \n",
       "5           0.17000         0.15780              0.08089         0.2087   \n",
       "6           0.10900         0.11270              0.07400         0.1794   \n",
       "7           0.16450         0.09366              0.05985         0.2196   \n",
       "8           0.19320         0.18590              0.09353         0.2350   \n",
       "9           0.23960         0.22730              0.08543         0.2030   \n",
       "\n",
       "   fractal_dimension_mean       ...         concave points_se  symmetry_se  \\\n",
       "0                 0.07871       ...                   0.01587      0.03003   \n",
       "1                 0.05667       ...                   0.01340      0.01389   \n",
       "2                 0.05999       ...                   0.02058      0.02250   \n",
       "3                 0.09744       ...                   0.01867      0.05963   \n",
       "4                 0.05883       ...                   0.01885      0.01756   \n",
       "5                 0.07613       ...                   0.01137      0.02165   \n",
       "6                 0.05742       ...                   0.01039      0.01369   \n",
       "7                 0.07451       ...                   0.01448      0.01486   \n",
       "8                 0.07389       ...                   0.01226      0.02143   \n",
       "9                 0.08243       ...                   0.01432      0.01789   \n",
       "\n",
       "   fractal_dimension_se  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0              0.006193         25.38          17.33           184.60   \n",
       "1              0.003532         24.99          23.41           158.80   \n",
       "2              0.004571         23.57          25.53           152.50   \n",
       "3              0.009208         14.91          26.50            98.87   \n",
       "4              0.005115         22.54          16.67           152.20   \n",
       "5              0.005082         15.47          23.75           103.40   \n",
       "6              0.002179         22.88          27.66           153.20   \n",
       "7              0.005412         17.06          28.14           110.60   \n",
       "8              0.003749         15.49          30.73           106.20   \n",
       "9              0.010080         15.09          40.68            97.65   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \n",
       "0      2019.0            0.1622             0.6656           0.7119  \n",
       "1      1956.0            0.1238             0.1866           0.2416  \n",
       "2      1709.0            0.1444             0.4245           0.4504  \n",
       "3       567.7            0.2098             0.8663           0.6869  \n",
       "4      1575.0            0.1374             0.2050           0.4000  \n",
       "5       741.6            0.1791             0.5249           0.5355  \n",
       "6      1606.0            0.1442             0.2576           0.3784  \n",
       "7       897.0            0.1654             0.3682           0.2678  \n",
       "8       739.3            0.1703             0.5401           0.5390  \n",
       "9       711.4            0.1853             1.0580           1.1050  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4 = X2.copy()\n",
    "X4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y4 = y2.copy()\n",
    "y4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X4_test, X4_train, y4_test, y4_train = train_test_split(X4,y4,test_size = 0.30,random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.svm import SVC\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create SVM classification object \n",
    "model = SVC(kernel = 'linear') # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X4, y4)\n",
    "model.score(X4, y4)\n",
    "#Predict Output\n",
    "predicted4= model.predict(X4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9597989949748744"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y4_test, predicted4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97       245\n",
      "          1       0.97      0.92      0.95       153\n",
      "\n",
      "avg / total       0.96      0.96      0.96       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y4_test, predicted4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Naive Bayes\n",
    "It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.\n",
    "\n",
    "Naive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:\n",
    "\n",
    "<img src=\"Bayes_rule.webp\">\n",
    "\n",
    "Here,\n",
    "\n",
    "* P(c|x) is the posterior probability of class (target) given predictor (attribute). \n",
    "* P(c) is the prior probability of class. \n",
    "* P(x|c) is the likelihood which is the probability of predictor given class. \n",
    "* P(x) is the prior probability of predictor.\n",
    "\n",
    "### Example: \n",
    "Let’s understand it using an example. Below I have a training data set of weather and corresponding target variable ‘Play’. Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.\n",
    "\n",
    "Step 1: Convert the data set to frequency table\n",
    "\n",
    "Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.\n",
    "\n",
    "<img src = \"Bayes_2.webp\">\n",
    "\n",
    "Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.\n",
    "\n",
    "Problem: Players will pay if weather is sunny, is this statement is correct?\n",
    "\n",
    "We can solve it using above discussed method, so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)\n",
    "\n",
    "Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64\n",
    "\n",
    "Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.\n",
    "\n",
    "Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = X2.copy()\n",
    "y5 = y2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 27), (171, 27), (398,), (171,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5,y5,test_size = 0.30 ,random_state = 10)\n",
    "X5_train.shape, X5_test.shape, y5_train.shape, y5_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X5, y5)\n",
    "#Predict Output\n",
    "predicted5 = model.predict(X5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y5_test,predicted5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92949251660224"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_score(y5_test,predicted5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.98       112\n",
      "          1       0.95      0.97      0.96        59\n",
      "\n",
      "avg / total       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "u =classification_report(y5_test,predicted5)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. kNN (k- Nearest Neighbors)\n",
    "It can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.\n",
    "\n",
    "These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing kNN modeling.\n",
    "\n",
    "KNN can easily be mapped to our real lives. If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!\n",
    "\n",
    "Things to consider before selecting kNN:\n",
    "\n",
    "* KNN is computationally expensive\n",
    "* Variables should be normalized else higher range variables can bias it\n",
    "* Works on pre-processing stage more before going for kNN like outlier, noise removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = X2.copy()\n",
    "y6 = y2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 27), (171, 27), (398,), (171,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6,y6, test_size = 0.30, random_state = 5)\n",
    "X6_train.shape, X6_test.shape, y6_train.shape, y6_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create KNeighbors classifier object model \n",
    "KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X6, y6)\n",
    "#Predict Output\n",
    "predicted6= model.predict(X6_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y6_test, predicted6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. K-Means\n",
    "\n",
    "## Unsupervised, Clustering Algorithm\n",
    "\n",
    "It is a type of unsupervised algorithm which  solves the clustering problem. Its procedure follows a simple and easy  way to classify a given data set through a certain number of  clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups.\n",
    "\n",
    "Remember figuring out shapes from ink blots? k means is somewhat similar this activity. You look at the shape and spread to decipher how many different clusters / population are present!\n",
    "\n",
    "### How K-means forms cluster:\n",
    "\n",
    "K-means picks k number of points for each cluster known as centroids.\n",
    "Each data point forms a cluster with the closest centroids i.e. k clusters.\n",
    "Finds the centroid of each cluster based on existing cluster members. Here we have new centroids.\n",
    "As we have new centroids, repeat step 2 and 3. Find the closest distance for each data point from new centroids and get associated with new k-clusters. Repeat this process until convergence occurs i.e. centroids does not change.\n",
    "\n",
    "### How to determine value of K:\n",
    "\n",
    "In K-means, we have clusters and each cluster has its own centroid. Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster. Also, when the sum of square values for all the clusters are added, it becomes total within sum of square value for the cluster solution.\n",
    "\n",
    "We know that as the number of cluster increases, this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k, and then much more slowly after that. Here, we can find the optimum number of cluster.\n",
    "\n",
    "<img src = 'Kmeans.webp'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X7 = X2.copy()\n",
    "y7 = y2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X7_train, X7_test, y7_train, y7_test = train_test_split(X7,y7, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checking the optimak number of clusters\n",
    "# from sklearn.decomposition import PCA \n",
    "# pca = PCA(n_components=8)\n",
    "# fit = pca.fit(X7)\n",
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# val = pca.explained_variance_ratio_\n",
    "# plt.plot(val)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=5,random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = k_means.fit(X7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 4, 0, 4, 2, 1, 4, 1, 0, 4,\n",
       "       0, 2, 1, 2, 2, 0, 0, 1, 2, 1, 0, 4, 2, 0, 0, 1, 0, 4, 0, 2, 4, 2,\n",
       "       0, 1, 2, 0, 2, 0, 4, 0, 1, 2, 2, 2, 2, 2, 2, 1, 0, 4, 2, 2, 4, 2,\n",
       "       1, 4, 1, 2, 0, 4, 2, 0, 1, 0, 2, 2, 2, 2, 4, 1, 4, 2, 4, 2, 0, 2,\n",
       "       4, 1, 2, 0, 0, 0, 2, 2, 1, 0, 2, 2, 2, 2, 0, 0, 1, 2, 4, 0, 0, 4,\n",
       "       0, 1, 4, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 4, 2,\n",
       "       0, 0, 2, 2, 0, 0, 3, 2, 2, 0, 1, 2, 2, 1, 0, 4, 2, 0, 2, 0, 2, 0,\n",
       "       2, 0, 0, 0, 2, 4, 1, 2, 2, 4, 2, 4, 0, 4, 0, 0, 2], dtype=int32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means.predict(X7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to do it is as below:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create KNeighbors classifier object model \n",
    "KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X7, y7)\n",
    "#Predict Output\n",
    "predicted7= model.predict(X7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.930393907748069"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing accuracy of the module\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "adjusted_rand_score(y7_test,predicted7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Random Forest\n",
    "Random Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n",
    "\n",
    "Each tree is planted & grown as follows:\n",
    "\n",
    "* If the number of cases in the training set is N, then sample of N cases is taken at random but with replacement. This sample will be the training set for growing the tree.\n",
    "* If there are M input variables, a number m<<M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.\n",
    "* Each tree is grown to the largest extent possible. There is no pruning.\n",
    "For more details on this algorithm, comparing with decision tree and tuning model parameters, I would suggest you to read these articles:\n",
    "\n",
    "With increase in computational power, we can now choose algorithms which perform very intensive calculations. One such algorithm is “Random Forest”, which we will discuss in this article. While the algorithm is very popular in various competitions (e.g. like the ones running on Kaggle), the end output of the model is like a black box and hence should be used judiciously.\n",
    "\n",
    "Before going any further, here is an example on the importance of choosing the best algorithm.\n",
    "\n",
    "## Case Study\n",
    "Following is a distribution of Annual income Gini Coefficients across different countries :\n",
    "\n",
    "<img src = 'oecd-income_inequality_2013_2.png'>\n",
    "\n",
    "Mexico has the second highest Gini coefficient and hence has a very high segregation in annual income of rich and poor. Our task is to come up with an accurate predictive algorithm to estimate annual income bracket of each individual in Mexico. The brackets of income are as follows :\n",
    "\n",
    "* 1. Below $40,000\n",
    "\n",
    "* 2. $40,000 – 150,000\n",
    "\n",
    "* 3. More than $150,000\n",
    "\n",
    "Following are the information available for each individual :\n",
    "\n",
    "1. Age , 2. Gender,  3. Highest educational qualification, 4. Working in Industry, 5. Residence in Metro/Non-metro\n",
    "\n",
    "We need to come up with an algorithm to give an accurate prediction for an individual who has following traits:\n",
    "\n",
    "1. Age : 35 years , 2, Gender : Male , 3. Highest Educational Qualification : Diploma holder, 4. Industry : Manufacturing, 5. Residence : Metro\n",
    "\n",
    "We will only talk about random forest to make this prediction in this article.\n",
    "\n",
    " \n",
    "\n",
    "### The algorithm of Random Forest\n",
    "\n",
    "Random forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART model with different sample and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction.\n",
    "\n",
    " \n",
    "\n",
    "## Back to Case  study\n",
    "#### Disclaimer : The numbers in this article are illustrative\n",
    "\n",
    "Mexico has a population of 118 MM. Say, the algorithm Random forest picks up 10k observation with only one variable (for simplicity) to build each CART model. In total, we are looking at 5 CART model being built with different variables. In a real life problem, you will have more number of population sample and different combinations of  input variables.\n",
    "\n",
    "#### Salary bands :\n",
    "\n",
    "Band 1 : Below $40,000\n",
    "\n",
    "Band 2: $40,000 – 150,000\n",
    "\n",
    "Band 3: More than $150,000\n",
    "\n",
    "Following are the outputs of the 5 different CART model.\n",
    "\n",
    "## CART 1 : Variable Age\n",
    "<img src = 'rf1.png'>\n",
    "\n",
    "## CART 2 : Variable Gender\n",
    "<img src = 'rf2.png'>\n",
    "\n",
    "## CART 3 : Variable Education\n",
    "<img src = 'rf3.png'>\n",
    "\n",
    "## CART 4 : Variable Residence\n",
    "\n",
    "<img src = 'rf4.png'>\n",
    "## CART 5 : Variable Industry\n",
    "<img src = 'rf5.png'>\n",
    "\n",
    "Using these 5 CART models, we need to come up with singe set of probability to belong to each of the salary classes. For simplicity, we will just take a mean of probabilities in this case study. Other than simple mean, we also consider vote method to come up with the final prediction. To come up with the final prediction let’s locate the following profile in each CART model :\n",
    "\n",
    "1. Age : 35 years , 2, Gender : Male , 3. Highest Educational Qualification : Diploma holder, 4. Industry : Manufacturing, 5. Residence : Metro\n",
    "\n",
    "For each of these CART model, following is the distribution across salary bands :\n",
    "\n",
    "<img src = 'DF.png'>\n",
    "\n",
    "The final probability is simply the average of the probability in the same salary bands in different CART models. As you can see from this analysis, that there is 70% chance of this individual falling in class 1 (less than $40,000) and around 24% chance of the individual falling in class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x8 = X2.copy()\n",
    "y8 = y2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 27), (171, 27), (398,), (171,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x8_train, x8_test, y8_train, y8_test = train_test_split(x8,y8, test_size = 0.30, random_state = 5)\n",
    "x8_train.shape, x8_test.shape, y8_train.shape, y8_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x8,y8)\n",
    "predicted8 = model.predict(x8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9941520467836257"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y8_test,predicted8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       110\n",
      "          1       1.00      0.98      0.99        61\n",
      "\n",
      "avg / total       0.99      0.99      0.99       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y8_test,predicted8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Algorithms\n",
    "In the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.\n",
    "\n",
    "For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.\n",
    "\n",
    "As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. How’d you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others.\n",
    "\n",
    "In May ‘ 2015, we conducted a Data Hackathon ( a data science competition) in Delhi-NCR, India.\n",
    "\n",
    "We gave participants the challenge to identify Human Activity Recognition Using Smartphones Data Set. The data set had 561 variables for training model used for the identification of Human activity in test data set.\n",
    "\n",
    "The participants in hackathon had varied experience and expertise level. As expected, the experts did a commendable job at identifying the human activity. However, beginners & intermediates struggled with sheer number of variables in the dataset (561 variables). Under the pressure of time, these people tried using variables  really without understanding the significance level of variable(s).  They lacked the skill to filter information from seemingly high dimensional problems and reduce them to a few relevant dimensions – the skill of dimension reduction.\n",
    "\n",
    "Further, this lack of skill came across in several forms in way of questions asked by various participants:\n",
    "\n",
    "There are too many variables – do I need to explore each and every variable?\n",
    "Are all variables important?\n",
    "All variables are numeric and what if they have multi-collinearity? How can I identify these variables?\n",
    "I want to use decision tree. It can automatically select the right variables. Is this a right technique?\n",
    "I am using random forest but it is taking a high execution time because of high number of features\n",
    "Is there any machine learning algorithm that can identify the most significant variables automatically?\n",
    "As this is a classification problem, can I use SVM with all variables?\n",
    "Which is the best tool to deal with high number of variable, R or Python?\n",
    "If you have faced similar questions, you are reading the right article. In this article, we will look at various methods to identify the significant variables using the most common dimension reduction *techniques and methods.*\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "Why Dimension Reduction is Important in machine learning and predictive modeling?\n",
    "What are Dimension Reduction techniques?\n",
    "What are the benefits of using Dimension Reduction techniques?\n",
    "What are the common methods to reduce number of Dimensions?\n",
    "Is Dimensionality Reduction good or bad?\n",
    "\n",
    "\n",
    "### Why Dimension Reduction is important in machine learning & predictive modeling?\n",
    "\n",
    "The problem of unwanted increase in dimension is closely related to fixation of measuring / recording data at a far granular level then it was done in past. This is no way suggesting that this is a recent problem. It has started gaining more importance lately due to surge in data.\n",
    "\n",
    "Lately, there has been a tremendous increase in the way sensors are being used in the industry. These sensors continuously record data and store it for analysis at a later point. In the way data gets captured, there can be a lot of redundancy. For example, let us take case of a motorbike rider in racing competitions. Today, his position and movement gets measured by GPS sensor on bike, gyro meters, multiple video feeds and his smart watch. Because of respective errors in recording, the data would not be exactly same. However, there is very little incremental information on position gained from putting these additional sources. Now assume that an analyst sits with all this data to analyze the racing strategy of the biker – he/ she would have a lot of variables / dimensions which are similar and of little (or no) incremental value. This is the problem of high unwanted dimensions and needs a treatment of dimension reduction.\n",
    "\n",
    "Let’s look at other examples of new ways of data collection:\n",
    "\n",
    "* Casinos are capturing data using cameras and tracking each and every move of their customers.\n",
    "* Political parties are capturing data by expanding their reach on field\n",
    "* Your smart phone apps collects a lot of personal details about you\n",
    "* Your set top box collects data about which programs preferences and timings\n",
    "* Organizations are evaluating their brand value by social media engagements (comments, likes), followers, positive and negative sentiments\n",
    "\n",
    "With more variables, comes more trouble! And to avoid this trouble, dimension reduction techniques comes to the rescue.\n",
    "\n",
    "### What are Dimension Reduction techniques?\n",
    "Dimension Reduction refers to the process of converting a set of data having vast dimensions into data with lesser dimensions ensuring that it conveys similar information concisely. These techniques are typically used while solving machine learning problems to obtain better features for a classification or regression task.\n",
    "\n",
    "Let’s look at the image shown below. It shows 2 dimensions x1 and x2, which are let us say measurements of several object in cm (x1) and inches (x2). Now, if you were to use both these dimensions in machine learning, they will convey similar information and introduce a lot of noise in system, so you are better of just using one dimension. Here we have converted the dimension of data from 2D (from x1 and x2) to 1D (z1), which has made the data relatively easier to explain.\n",
    "\n",
    "In similar ways, we can reduce n dimensions of data set to k dimensions (k < n) . These k dimensions can be directly identified (filtered) or can be a combination of dimensions (weighted averages of dimensions) or new dimension(s) that represent existing multiple dimensions well.\n",
    "\n",
    "\n",
    "What are the benefits of Dimension Reduction?\n",
    "Let’s look at the benefits of applying Dimension Reduction process:\n",
    "\n",
    "* It helps in data compressing and reducing the storage space required\n",
    "* It fastens the time required for performing same computations. Less dimensions leads to less computing, also less dimensions can allow usage of algorithms unfit for a large number of dimensions\n",
    "* It takes care of multi-collinearity that improves the model performance. It removes redundant features. For example: there is no point in storing a value in two different units (meters and inches).\n",
    "* Reducing the dimensions of data to 2D or 3D may allow us to plot and visualize it precisely. You can then observe patterns more clearly. Below you can see that, how a 3D data is converted into 2D. First it has identified the 2D plane then represented the points on these two new axis z1 and z2.\n",
    "* It is helpful in noise removal also and as result of that we can improve the performance of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the common methods to perform Dimension Reduction?\n",
    "\n",
    "There are many methods to perform Dimension reduction. I have listed the most common methods below:\n",
    "\n",
    "1. Missing Values: While exploring data, if we encounter missing values, what we do? Our first step should be to identify the reason then impute missing values/ drop variables using appropriate methods. But, what if we have too many missing values? Should we impute missing values or drop the variables?\n",
    "\n",
    "I would prefer the latter, because it would not have lot more details about data set. Also, it would not help in improving the power of model. Next question, is there any threshold of missing values for dropping a variable? It varies from case to case. If the information contained in the variable is not that much, you can drop the variable if it has more than ~40-50% missing values.\n",
    "\n",
    "2. Low Variance: Let’s think of a scenario where we have a constant variable (all observations have same value, 5) in our data set. Do you think, it can improve the power of model? Ofcourse NOT, because it has zero variance. In case of high number of dimensions, we should drop variables having low variance compared to others because these variables will not explain the variation in target variables.\n",
    "\n",
    "3. Decision Trees: It is one of my favorite techniques. It can be used as a ultimate solution to tackle multiple challenges like missing values, outliers and identifying significant variables. It worked well in our Data Hackathon also. Several data scientists used decision tree and it worked well for them.\n",
    "\n",
    "4. Random Forest: Similar to decision tree is Random Forest. I would also recommend using the in-built feature importance provided by random forests to select a smaller subset of input features. Just be careful that random forests have a tendency to bias towards variables that have more no. of distinct values i.e. favor numeric variables over binary/categorical values.\n",
    "\n",
    " \n",
    "5. High Correlation: Dimensions exhibiting higher correlation can lower down the performance of model. Moreover, it is not good to have multiple variables of similar information or variation also known as “Multicollinearity”. You can use Pearson (continuous variables) or Polychoric (discrete variables) correlation matrix to identify the variables with high correlation and select one of them using VIF (Variance Inflation Factor). Variables having higher value ( VIF > 5 ) can be dropped.\n",
    "\n",
    " \n",
    "\n",
    "6. Backward Feature Elimination: In this method, we start with all n dimensions. Compute the sum of square of error (SSR) after eliminating each variable (n times). Then, identifying variables whose removal has produced the smallest increase in the SSR and removing it finally, leaving us with n-1 input features.\n",
    "\n",
    "Repeat this process until no other variables can be dropped. Recently in Online Hackathon organised by Analytics Vidhya (11-12 Jun’15), Data scientist who held second position used Backward Feature Elimination in linear regression to train his model.\n",
    "Reverse to this, we can use “Forward Feature Selection” method. In this method, we select one variable and analyse the performance of model by adding another variable. Here, selection of variable is based on higher improvement in model performance.\n",
    "\n",
    "\n",
    "7. Factor Analysis: Let’s say some variables are highly correlated. These variables can be grouped by their correlations i.e. all variables in a particular group can be highly correlated among themselves but have low correlation with variables of other group(s). Here each group represents a single underlying construct or factor. These factors are small in number as compared to large number of dimensions. However, these factors are difficult to observe. There are basically two methods of performing factor analysis:\n",
    "\n",
    ">>EFA (Exploratory Factor Analysis)\n",
    "\n",
    ">>CFA (Confirmatory Factor Analysis)\n",
    "\n",
    "8. Principal Component Analysis (PCA): In this technique, variables are transformed into a new set of variables, which are linear combination of original variables. These new set of variables are known as principle components. They are obtained in such a way that first principle component accounts for most of the possible variation of original data after which each succeeding component has the highest possible variance.\n",
    "\n",
    "The second principal component must be orthogonal to the first principal component. In other words, it does its best to capture the variance in the data that is not captured by the first principal component. For two-dimensional dataset, there can be only two principal components. Below is a snapshot of the data and its first and second principal components. You can notice that second principle component is orthogonal to first principle component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x9 = X2.copy()\n",
    "y9 = y2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01137</td>\n",
       "      <td>0.02165</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01039</td>\n",
       "      <td>0.01369</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01448</td>\n",
       "      <td>0.01486</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01226</td>\n",
       "      <td>0.02143</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01432</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "5        12.45         15.70           82.57      477.1          0.12780   \n",
       "6        18.25         19.98          119.60     1040.0          0.09463   \n",
       "7        13.71         20.83           90.20      577.9          0.11890   \n",
       "8        13.00         21.82           87.50      519.8          0.12730   \n",
       "9        12.46         24.04           83.97      475.9          0.11860   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760         0.30010              0.14710         0.2419   \n",
       "1           0.07864         0.08690              0.07017         0.1812   \n",
       "2           0.15990         0.19740              0.12790         0.2069   \n",
       "3           0.28390         0.24140              0.10520         0.2597   \n",
       "4           0.13280         0.19800              0.10430         0.1809   \n",
       "5           0.17000         0.15780              0.08089         0.2087   \n",
       "6           0.10900         0.11270              0.07400         0.1794   \n",
       "7           0.16450         0.09366              0.05985         0.2196   \n",
       "8           0.19320         0.18590              0.09353         0.2350   \n",
       "9           0.23960         0.22730              0.08543         0.2030   \n",
       "\n",
       "   fractal_dimension_mean       ...         concave points_se  symmetry_se  \\\n",
       "0                 0.07871       ...                   0.01587      0.03003   \n",
       "1                 0.05667       ...                   0.01340      0.01389   \n",
       "2                 0.05999       ...                   0.02058      0.02250   \n",
       "3                 0.09744       ...                   0.01867      0.05963   \n",
       "4                 0.05883       ...                   0.01885      0.01756   \n",
       "5                 0.07613       ...                   0.01137      0.02165   \n",
       "6                 0.05742       ...                   0.01039      0.01369   \n",
       "7                 0.07451       ...                   0.01448      0.01486   \n",
       "8                 0.07389       ...                   0.01226      0.02143   \n",
       "9                 0.08243       ...                   0.01432      0.01789   \n",
       "\n",
       "   fractal_dimension_se  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0              0.006193         25.38          17.33           184.60   \n",
       "1              0.003532         24.99          23.41           158.80   \n",
       "2              0.004571         23.57          25.53           152.50   \n",
       "3              0.009208         14.91          26.50            98.87   \n",
       "4              0.005115         22.54          16.67           152.20   \n",
       "5              0.005082         15.47          23.75           103.40   \n",
       "6              0.002179         22.88          27.66           153.20   \n",
       "7              0.005412         17.06          28.14           110.60   \n",
       "8              0.003749         15.49          30.73           106.20   \n",
       "9              0.010080         15.09          40.68            97.65   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \n",
       "0      2019.0            0.1622             0.6656           0.7119  \n",
       "1      1956.0            0.1238             0.1866           0.2416  \n",
       "2      1709.0            0.1444             0.4245           0.4504  \n",
       "3       567.7            0.2098             0.8663           0.6869  \n",
       "4      1575.0            0.1374             0.2050           0.4000  \n",
       "5       741.6            0.1791             0.5249           0.5355  \n",
       "6      1606.0            0.1442             0.2576           0.3784  \n",
       "7       897.0            0.1654             0.3682           0.2678  \n",
       "8       739.3            0.1703             0.5401           0.5390  \n",
       "9       711.4            0.1853             1.0580           1.1050  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x9.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y9[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 27), (114, 27), (455,), (114,))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x9_train, x9_test, y9_train, y9_test = train_test_split(x9,y9, test_size = 0.2, random_state= 1234)\n",
    "x9_train.shape, x9_test.shape, y9_train.shape, y9_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model = PCA(n_components= 10,whiten=True, tol= 0.4, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True,\n",
       " 'iterated_power': 'auto',\n",
       " 'n_components': 10,\n",
       " 'random_state': 1234,\n",
       " 'svd_solver': 'auto',\n",
       " 'tol': 0.4,\n",
       " 'whiten': True}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check the parameters we can change in this model\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=10, random_state=1234,\n",
       "  svd_solver='auto', tol=0.4, whiten=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let us fit the model now and see:\n",
    "model.fit(x9,y9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the dimensions of test and train\n",
    "train_reduced = model.fit_transform(x9_train,y9_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39630535, -0.35143118,  0.34482807, ...,  0.28332037,\n",
       "         0.4985502 , -0.6342089 ],\n",
       "       [-0.79802171, -0.0785324 ,  0.23554171, ...,  0.05156462,\n",
       "        -0.33181675, -0.45196981],\n",
       "       [-0.59896309,  0.36911938, -0.00330755, ...,  1.28286509,\n",
       "        -0.36380683,  0.00466264],\n",
       "       ...,\n",
       "       [-0.4016872 , -0.47900239,  0.21638678, ..., -0.44507672,\n",
       "         0.85890525, -0.05576649],\n",
       "       [ 0.87304025,  1.0675429 ,  1.03066507, ..., -0.24664661,\n",
       "        -0.08509915,  1.22927685],\n",
       "       [-0.55643337,  0.32696579, -0.32475448, ..., -0.39264916,\n",
       "        -0.53607077, -0.00221829]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Algorithms\n",
    "\n",
    "GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.\n",
    "\n",
    "There are some machine learning engines. These engines make use of certain algorithms and help user reach to the output stage. Some of the most popular engines are Decision Tree and Regression.\n",
    "\n",
    "In this article, we’ll introduce you to some of the best practices used to enhance power of these engines to achieve a higher predictability using an additional booster.\n",
    "\n",
    "\n",
    "\n",
    "#### Where are Boosted algorithms required?\n",
    "Boosted algorithms are used where we have plenty of data to make a prediction. And we seek exceptionally high predictive power. It is used to for reducing bias and variance in supervised learning. It combines multiple weak predictors to a build strong predictor.\n",
    "\n",
    "If you ever want to participate in Kaggle competitions, I would suggest that you bookmark this article. Participants in Kaggle completitions use these boosting algorithms extensively.\n",
    "\n",
    "The underlying engine used for boosting algorithms can be anything. For instance, AdaBoost is a boosting done on Decision stump. There are many other boosting algorithms which use other types of engine such as:\n",
    "\n",
    "1. GentleBoost\n",
    "\n",
    "2. Gradient Boosting (Always my first choice for any Kaggle problem)\n",
    "\n",
    "3.  LPBoost\n",
    "\n",
    "4. BrownBoost\n",
    "\n",
    "Perhaps, I can go on adding more engines to this list. But, I would like to focus on these five boosting techniques which are the most commonly used. **Let’s first learn about – AdaBoost.**\n",
    "\n",
    "\n",
    "## What are Classifier Boosting Algorithms ?\n",
    "Classification problem is the one where we need to assign every observation to a given set of class. The easiest classification problem is the one with binary class. This problem can be solved using AdaBoost. Let’s take a very simple example to understand the underlying concept of AdaBoost. You have two classes : 0’s and 1’s. Each number is an observation. The only two features available is x-axis and y-axis. For instance (1,1)  is a 0 while (4,4) is a 1. Now using these two features you need to classify each observation. Our ultimate objective remains the same as any classifier problem : find the classification boundary. Following are the step we follow to apply an AdaBoost.\n",
    "\n",
    "**Step 1 : Visualize the data :** Let’s first understand the data and find insights on whether we have a linear classifier boundary. As shown below, no such boundary exist which can separate 0’s from 1’s.\n",
    "\n",
    "<img src = 'gb1.png'>\n",
    "\n",
    "\n",
    "**Step 2 : Make the first Decision stump :** You have already read about decision trees in many of our previous articles. Decision stump is a unit depth tree which decides just 1 most significant cut on features. Here it chooses draw the boundary starting from the third row from top. Now the yellow portion is expected to be all 0’s and unshaded portion to be all 1’s. However, we see high number of false positive post we build this decision stump. We have nine 1’s being wrongly qualified as 0’s. And similarly eighteen 0’s qualified as 1’s.\n",
    "\n",
    "<img src = 'pgb2.webp'>\n",
    "\n",
    "**Step 3 : Give additional weight to mis-classified observations:** Once we know the misclassified observations, we give additional weight to these observations. Hence, you see 0’s and 1’s in bold which were misclassified before. In the next level, we will make sure that these highly weighted observation are classified correct\n",
    "\n",
    "<img src = 'gb3.webp'>\n",
    "\n",
    "**Step 4 : Repeat the process and combine all stumps to get final classifier :** We repeat the process multiple times and focus more on previously misclassified observations. Finally, we take a weighted mean of all the boudaries discovered which will look something as below.\n",
    "\n",
    "<img src = 'gb4.webp'>\n",
    "\n",
    "### Brief introduction to Regression boosters\n",
    "Similar to classifier boosters, we also have regression boosters. In these problems we have continuous variable to predict. This is commonly done using gradient boosting algorithm. Here is a non-mathematical description of how gradient boost works :\n",
    "\n",
    "Type of Problem – You have a set of variables vectors x1 , x2 and x3. You need to predict y which is a continuous variable.\n",
    "\n",
    "Steps of Gradient Boost algorithm\n",
    "\n",
    "* Step 1 : Assume mean is the prediction of all variables.\n",
    "\n",
    "* Step 2 : Calculate errors of each observation from the mean (latest prediction).\n",
    "\n",
    "* Step 3 : Find the variable that can split the errors perfectly and find the value for the split. This is assumed to be the latest prediction.\n",
    "\n",
    "* Step 4 : Calculate errors of each observation from the mean of both the sides of split (latest prediction).\n",
    "\n",
    "* Step 5 : Repeat the step 3 and 4 till the objective function maximizes/minimizes.\n",
    "\n",
    "* Step 6 : Take a weighted mean of all the classifiers to come up with the final model.\n",
    "\n",
    "We have excluded the mathematical formation of boosting algorithms from this article to keep the article simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x10 = X2.copy()\n",
    "y10 = y2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x10_train, x10_test,y10_train,y10_test = train_test_split(x10,y10, test_size = 0.30, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create Gradient Boosting Classifier object\n",
    "model2= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "# Train the model using the training sets and check score\n",
    "model2.fit(x10, y10)\n",
    "#Predict Output\n",
    "predicted10= model2.predict(x10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y10_test,predicted10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href = 'https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/'>Hyper Parameter Tuning XGBoost</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 1,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'presort': 'auto',\n",
       " 'random_state': 0,\n",
       " 'subsample': 1.0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#supposing I play with the hyper parameters in the model\n",
    "model2.get_params()\n",
    "#below are the parameters we can play with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of XGBoost\n",
    "\n",
    "I’ve always admired the boosting capabilities that this algorithm infuses in a predictive model. When I explored more about its performance and science behind its high accuracy, I discovered many advantages:\n",
    "\n",
    "* Regularization:\n",
    ">Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.\n",
    "\n",
    ">In fact, XGBoost is also known as ‘regularized boosting‘ technique.\n",
    "\n",
    "* Parallel Processing:\n",
    ">XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "\n",
    ">But hang on, we know that boosting is sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.\n",
    "\n",
    ">XGBoost also supports implementation on Hadoop.\n",
    "\n",
    "* High Flexibility\n",
    ">XGBoost allow users to define custom optimization objectives and evaluation criteria.\n",
    "\n",
    ">This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "\n",
    "* Handling Missing Values\n",
    "> XGBoost has an in-built routine to handle missing values.\n",
    "\n",
    ">User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
    "\n",
    "\n",
    "* Tree Pruning:\n",
    ">A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
    "\n",
    ">XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "\n",
    ">Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "\n",
    "* Built-in Cross-Validation\n",
    ">XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "\n",
    ">This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
    "\n",
    "* Continue on Existing Model\n",
    ">User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.\n",
    "\n",
    ">GBM implementation of sklearn also has this feature so they are even on this point.\n",
    "I hope now you understand the sheer power XGBoost algorithm. Note that these are the points which I could muster. You know a few more? Feel free to drop a comment below and I will update the list.\n",
    "\n",
    "Did I whet your appetite ? Good. You can refer to following web-pages for a deeper understanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I landed on some links that have some useful information about the topic above:\n",
    "\n",
    "Studied various articles on handling data:\n",
    " 1- <a href ='https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python' >Cool Article</a>\n",
    " 2- <a href ='https://www.kaggle.com/nanomathias/feature-engineering-importance-testing' >Feature Engineering</a>\n",
    " 3- <a href ='https://www.kaggle.com/asindico/customer-segments-with-pca' >Principle Component Analysis</a>\n",
    " 4- <a href ='https://www.kaggle.com/dansbecker/cross-validation' >Cross Validation</a>\n",
    " 5- <a href ='https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search' >Hyper parameter tuning</a>\n",
    " 6- <a href ='https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/' >XBoost</a>\n",
    "  7- <a href ='https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/#' >complete-guide-parameter-tuning-gradient-boosting-gbm-python/#</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost\n",
    "CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML.\n",
    "\n",
    "The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be.\n",
    "\n",
    "Make sure you handle missing data well before you proceed with the implementation.\n",
    "\n",
    "Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.\n",
    "\n",
    ">> 'Error: could not convert string to float:'\n",
    "\n",
    "This error occurs when dealing with categorical (string) variables. In sklearn, you are required to convert these categories in the numerical format.\n",
    "\n",
    "In order to do this conversion, we use several pre-processing methods like “label encoding”, “one hot encoding” and others.\n",
    "\n",
    "In this article, I will discuss a recently open sourced library ” CatBoost” developed and contributed by Yandex. CatBoost can use categorical features directly and is scalable in nature.\n",
    "\n",
    "### What is CatBoost?\n",
    "CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.\n",
    "\n",
    "It is especially powerful in two ways:\n",
    "\n",
    ">>It yields state-of-the-art results without extensive data training typically required by other machine learning methods, and\n",
    "\n",
    ">>Provides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems.\n",
    "\n",
    "“CatBoost” name comes from two words “Category” and “Boosting”.\n",
    "\n",
    "As discussed, the library works well with multiple Categories of data, such as audio, text, image including historical data.\n",
    "\n",
    "“Boost” comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n",
    "\n",
    "\n",
    "### Advantages of CatBoost Library\n",
    "* **Performance:** CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front.\n",
    "* **Handling Categorical features automatically:** We can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features. You can read more about it here.\n",
    "* **Robust:** It reduces the need for extensive hyper-parameter tuning and lower the chances of overfitting also which leads to more generalized models. Although, CatBoost has multiple parameters to tune and it contains parameters like the number of trees, learning rate, regularization, tree depth, fold size, bagging temperature and others. You can read about all these parameters here.\n",
    "* **Easy-to-use:** You can use CatBoost from the command line, using an user-friendly API for both Python and R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BackUP/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Volumes/BackUP/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Volumes/BackUP/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Volumes/BackUP/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1bbb15d97c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mXx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myx_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#categorical_features_indices = np.where(X.dtypes != np.float)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X5' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xx_train, Xx_validation, yx_train, yx_validation = train_test_split(X5, y5, train_size=0.7, random_state=1234)\n",
    "#categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "#importing library and building model\n",
    "from catboost import CatBoostRegressormodel\n",
    "modelX=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n",
    "\n",
    "modelX.fit(Xx_train, yx_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)\n",
    "modelX.predict(yx_validation)\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['Item_Identifier'] = test['Item_Identifier']\n",
    "submission['Outlet_Identifier'] = test['Outlet_Identifier']\n",
    "submission['Item_Outlet_Sales'] = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
